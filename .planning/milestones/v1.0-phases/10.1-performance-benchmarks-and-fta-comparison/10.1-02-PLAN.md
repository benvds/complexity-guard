---
phase: 10.1-performance-benchmarks-and-fta-comparison
plan: 02
type: execute
wave: 2
depends_on: ["10.1-01"]
files_modified:
  - benchmarks/src/benchmark.zig
  - benchmarks/scripts/bench-subsystems.sh
  - build.zig
autonomous: true
requirements:
  - "BENCH-SUBSYS: Zig-built benchmark module for subsystem profiling (parsing, analysis, output)"

must_haves:
  truths:
    - "Running `zig build bench` compiles the benchmark module with ReleaseFast optimization"
    - "Running bench-subsystems.sh on a cloned project produces per-subsystem timing breakdown (file discovery, parsing, cyclomatic, cognitive, halstead, structural, scoring, output)"
    - "Subsystem timings use nanosecond-precision monotonic timer (std.time.Timer)"
    - "Results are printed in structured format parseable for documentation"
    - "Benchmark module exercises the real CG pipeline stages individually (not mocked)"
  artifacts:
    - path: "benchmarks/src/benchmark.zig"
      provides: "Zig subsystem benchmark module exercising each pipeline stage independently"
      min_lines: 100
    - path: "benchmarks/scripts/bench-subsystems.sh"
      provides: "Shell wrapper that runs Zig benchmark against cloned projects"
    - path: "build.zig"
      provides: "Updated with 'bench' build step"
      contains: "b.step(\"bench\""
  key_links:
    - from: "benchmarks/src/benchmark.zig"
      to: "src/discovery/walker.zig"
      via: "import and call walker functions"
      pattern: "@import.*walker"
    - from: "benchmarks/src/benchmark.zig"
      to: "src/parser/parse.zig"
      via: "import and call parse functions"
      pattern: "@import.*parse"
    - from: "benchmarks/src/benchmark.zig"
      to: "src/metrics/"
      via: "import and call each metric analyzer"
      pattern: "@import.*metrics"
    - from: "build.zig"
      to: "benchmarks/src/benchmark.zig"
      via: "addExecutable with bench root_source_file"
      pattern: "benchmark\\.zig"
---

<objective>
Create a Zig-native subsystem benchmark module that profiles each pipeline stage (file discovery, parsing, cyclomatic analysis, cognitive analysis, halstead analysis, structural analysis, scoring, output serialization) independently. This identifies bottlenecks and provides the granular timing data needed for Phase 12 parallelization design.

Purpose: End-to-end hyperfine benchmarks (Plan 01) show total runtime; this plan reveals WHERE time is spent internally. Critical for making informed decisions about what to parallelize in Phase 12.

Output: benchmarks/src/benchmark.zig, bench build step in build.zig, bench-subsystems.sh wrapper script.
</objective>

<execution_context>
@/home/ben/.claude/get-shit-done/workflows/execute-plan.md
@/home/ben/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10.1-performance-benchmarks-and-fta-comparison/10.1-CONTEXT.md
@.planning/phases/10.1-performance-benchmarks-and-fta-comparison/10.1-RESEARCH.md
@.planning/phases/10.1-performance-benchmarks-and-fta-comparison/10.1-01-SUMMARY.md
@build.zig
@src/main.zig
@src/discovery/walker.zig
@src/parser/parse.zig
@src/metrics/cyclomatic.zig
@src/metrics/cognitive.zig
@src/metrics/halstead.zig
@src/metrics/structural.zig
@src/metrics/scoring.zig
@src/output/json_output.zig
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Zig subsystem benchmark module and add bench step to build.zig</name>
  <files>
    benchmarks/src/benchmark.zig
    build.zig
  </files>
  <action>
    1. **Update build.zig** to add a "bench" build step:
       - Create a new executable: `b.addExecutable(.{ .name = "complexity-bench", .root_module = b.createModule(.{ .root_source_file = b.path("benchmarks/src/benchmark.zig"), .target = target, .optimize = optimize }) })`
       - Add the same dependencies as the main exe: toml import, tree-sitter C sources via `addTreeSitterSources(b, bench_exe)`
       - Add the main source as an importable module so benchmark.zig can `@import("complexity_guard")` for real pipeline functions. Alternatively, add `src/` path as module root so benchmark can import individual modules.
       - Actually, the cleanest approach: use `bench_exe.root_module.addImport("toml", toml_dep.module("toml"))` and add include paths for source files. The benchmark module will import from `src/` using relative paths from its own location. Since `root_source_file` is `benchmarks/src/benchmark.zig`, you need to use `addImport` to expose the main module. **Recommended approach:** Add main.zig's module as a dependency:
         ```zig
         const main_module = b.createModule(.{
             .root_source_file = b.path("src/main.zig"),
             .target = target,
             .optimize = optimize,
         });
         main_module.addImport("toml", toml_dep.module("toml"));
         // Add C includes to main_module too if needed
         bench_exe.root_module.addImport("complexity_guard", main_module);
         ```
         Or simpler: just point the benchmark's root_source_file directly at a file in src/ that re-exports what's needed, or make benchmark.zig use the same module root as src/.

       - **Simplest correct approach:** Make the bench executable's root at `benchmarks/src/benchmark.zig` and add a module import for each needed source file, OR use the build system to make `src/` available as a dependency. The most pragmatic approach for Zig 0.15.2: add the source directory as an anonymous import. Let's use this approach:
         ```zig
         const bench_mod = b.createModule(.{
             .root_source_file = b.path("benchmarks/src/benchmark.zig"),
             .target = target,
             .optimize = optimize,
         });
         bench_mod.addImport("toml", toml_dep.module("toml"));
         // Import individual source modules needed by benchmark
         const walker_mod = b.createModule(.{ .root_source_file = b.path("src/discovery/walker.zig"), .target = target, .optimize = optimize });
         const parse_mod = b.createModule(.{ .root_source_file = b.path("src/parser/parse.zig"), .target = target, .optimize = optimize });
         // ... etc for each module
         bench_mod.addImport("walker", walker_mod);
         bench_mod.addImport("parse", parse_mod);
         ```
         This gets complex. **Best approach:** Create a single module from src/main.zig and import it as a namespace. Check how the existing codebase structure works first (read main.zig imports).

       - **IMPORTANT: Read src/main.zig first** to understand the module structure and how internal modules reference each other. Then decide the cleanest way to expose pipeline functions to the benchmark module. The executor should study the import graph before committing to an approach.

       - Install the bench artifact and create the bench step:
         ```zig
         b.installArtifact(bench_exe);
         const bench_run = b.addRunArtifact(bench_exe);
         bench_run.step.dependOn(b.getInstallStep());
         if (b.args) |args| bench_run.addArgs(args);
         const bench_step = b.step("bench", "Run subsystem benchmarks");
         bench_step.dependOn(&bench_run.step);
         ```

    2. **Create benchmarks/src/benchmark.zig**:
       - Accept a directory path as command-line argument (the project to benchmark)
       - Accept optional `--runs N` flag (default 5) for number of iterations per subsystem
       - Use `std.time.Timer` for all measurements (nanosecond precision, monotonic)

       - **Subsystem measurement flow** (each measured independently):
         a. **File Discovery**: Time `walker` discovering .ts/.tsx/.js/.jsx files in the target directory. Record file count.
         b. **File I/O (Read)**: Time reading all discovered file contents into memory. Record total bytes.
         c. **Parsing**: Time `parse` module parsing all files into ASTs via tree-sitter. Record parse success/fail counts.
         d. **Cyclomatic Analysis**: Time cyclomatic complexity analysis across all parsed files. Record function count.
         e. **Cognitive Analysis**: Time cognitive complexity analysis across all parsed files.
         f. **Halstead Analysis**: Time Halstead metric computation across all parsed files.
         g. **Structural Analysis**: Time structural metric computation across all parsed files.
         h. **Scoring**: Time health score computation across all results.
         i. **JSON Serialization**: Time JSON output generation for all results.

       - For each subsystem, run N iterations, collect min/max/mean/stddev.
       - Between subsystem measurements, reuse artifacts from previous stages (e.g., parsed trees for analysis benchmarks).

       - **Output format**: Print structured text to stdout:
         ```
         Benchmark: <project_path>
         Files: <N>  Functions: <N>  Bytes: <N>
         Runs: <N>

         Subsystem          Mean (ms)    Stddev (ms)   Min (ms)     Max (ms)
         ─────────────────────────────────────────────────────────────────────
         file_discovery      1.234        0.012         1.220        1.248
         file_read           2.345        0.034         2.310        2.380
         parsing             15.678       0.234         15.444       15.912
         cyclomatic          3.456        0.056         3.400        3.512
         cognitive           4.567        0.067         4.500        4.634
         halstead            8.901        0.123         8.778        9.024
         structural          2.345        0.045         2.300        2.390
         scoring             0.123        0.002         0.121        0.125
         json_output         5.678        0.089         5.589        5.767
         ─────────────────────────────────────────────────────────────────────
         Total pipeline      44.327       0.662

         Hotspot: parsing (35.4% of total)
         ```

       - Also output JSON to a file if `--json <path>` flag is provided, using the schema:
         ```json
         {
           "project": "<path>",
           "files": N, "functions": N, "bytes": N,
           "runs": N,
           "subsystems": {
             "file_discovery": { "mean_ms": 1.234, "stddev_ms": 0.012, "min_ms": 1.220, "max_ms": 1.248 },
             ...
           },
           "total_pipeline_mean_ms": 44.327,
           "hotspot": "parsing",
           "hotspot_pct": 35.4
         }
         ```

       - Use ArenaAllocator for the entire benchmark run (short-lived process).
       - Properly handle tree-sitter cleanup (defer ts_tree_delete for each parsed tree).
       - If `benchmarks/projects/` is empty or target dir doesn't exist, print a clear error: "No benchmark projects found. Run: benchmarks/scripts/setup.sh --suite quick"
  </action>
  <verify>
    - `zig build bench` compiles successfully
    - `zig build bench -- benchmarks/projects/zod` (or any cloned project) produces per-subsystem timing output
    - Output shows all 9 subsystems with mean/stddev/min/max in milliseconds
    - `zig build bench -- --help` or missing argument prints usage
    - `zig build bench -Doptimize=ReleaseFast -- benchmarks/projects/zod` works with optimization
  </verify>
  <done>
    Zig benchmark module compiles via `zig build bench`, accepts a project directory, and profiles all 9 pipeline subsystems with statistical timing (mean/stddev/min/max). Outputs human-readable table and optional JSON.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create bench-subsystems.sh wrapper script</name>
  <files>
    benchmarks/scripts/bench-subsystems.sh
  </files>
  <action>
    1. Create `benchmarks/scripts/bench-subsystems.sh`:
       - `set -euo pipefail`
       - Determine PROJECT_ROOT via `git rev-parse --show-toplevel`
       - Build CG benchmark in ReleaseFast: `cd "$PROJECT_ROOT" && zig build bench -Doptimize=ReleaseFast`
       - Accept `--suite` flag: `quick` (default), `full`, `stress` (same tiers as setup.sh)
       - Create timestamped results directory: `RESULTS_DIR="$PROJECT_ROOT/benchmarks/results/baseline-$(date +%Y-%m-%d)"`
       - For each project in the selected suite:
         ```
         zig-out/bin/complexity-bench \
           --runs 10 \
           --json "$RESULTS_DIR/${project}-subsystems.json" \
           "$PROJECT_ROOT/benchmarks/projects/${project}"
         ```
       - Print summary at end: which subsystem is the hotspot across all projects (aggregate the hotspot_pct values)
       - Make executable with `chmod +x`

    2. The script should:
       - Check that projects are cloned (error with suggestion to run setup.sh)
       - Print progress: "Benchmarking subsystems: <project> (<N> of <M>)"
       - Handle missing projects gracefully (skip with warning, don't abort entire run)
       - Use `--runs 10` for quick/full, `--runs 3` for stress suite
  </action>
  <verify>
    - `bash benchmarks/scripts/bench-subsystems.sh` runs on quick suite and produces subsystem JSON files
    - Each project gets a `*-subsystems.json` file in the results directory
    - Script prints progress and summary
    - Script fails gracefully if projects aren't cloned (clear error message)
  </verify>
  <done>
    bench-subsystems.sh runs the Zig benchmark module against all projects in the selected suite, producing per-project subsystem timing JSON files. Summary identifies the overall hotspot subsystem across all projects.
  </done>
</task>

</tasks>

<verification>
1. `zig build bench` compiles without errors
2. `zig build bench -Doptimize=ReleaseFast -- benchmarks/projects/zod` produces subsystem timing table
3. `bash benchmarks/scripts/bench-subsystems.sh` produces JSON files in results directory
4. build.zig has a "bench" step that doesn't interfere with existing "run" and "test" steps
5. Benchmark module correctly imports and exercises real CG pipeline stages (not mocked)
</verification>

<success_criteria>
- Zig subsystem benchmark module profiles all 9 pipeline stages independently with statistical rigor
- build.zig has a working "bench" step that compiles the benchmark in ReleaseFast
- bench-subsystems.sh automates running subsystem benchmarks across project suites
- Output clearly identifies which subsystem dominates runtime (parsing? analysis? output?)
</success_criteria>

<output>
After completion, create `.planning/phases/10.1-performance-benchmarks-and-fta-comparison/10.1-02-SUMMARY.md`
</output>
