---
phase: 10.1-performance-benchmarks-and-fta-comparison
plan: 03
type: execute
wave: 2
depends_on: ["10.1-01"]
files_modified:
  - benchmarks/scripts/compare-metrics.sh
  - benchmarks/scripts/compare_metrics.py
  - benchmarks/scripts/summarize_results.py
  - benchmarks/README.md
  - docs/benchmarks.md
  - README.md
  - publication/npm/README.md
  - publication/npm/packages/darwin-arm64/README.md
  - publication/npm/packages/darwin-x64/README.md
  - publication/npm/packages/linux-arm64/README.md
  - publication/npm/packages/linux-x64/README.md
  - publication/npm/packages/windows-x64/README.md
autonomous: true
requirements:
  - "BENCH-ACCURACY: Metric accuracy comparison between CG and FTA on overlapping metrics"
  - "BENCH-DOCS: Exhaustive documentation of methodology, results, and conclusions"

must_haves:
  truths:
    - "compare-metrics.sh runs both CG and FTA on same files, producing a per-file metric comparison JSON"
    - "Metric comparison documents the granularity difference (FTA=file-level, CG=function-level) and aggregation method"
    - "docs/benchmarks.md explains methodology clearly enough for someone to reproduce results"
    - "docs/benchmarks.md summarizes key findings with speed and memory comparisons"
    - "benchmarks/README.md documents how to run all benchmark scripts and interpret results"
    - "Benchmark data is structured for trivial before/after comparison in Phase 11/12"
    - "README.md updated to mention benchmarks in features or documentation links"
  artifacts:
    - path: "benchmarks/scripts/compare-metrics.sh"
      provides: "Shell script that runs both tools and feeds output to Python comparison"
    - path: "benchmarks/scripts/compare_metrics.py"
      provides: "Python script comparing CG vs FTA metric values with tolerance bands"
    - path: "benchmarks/scripts/summarize_results.py"
      provides: "Python script that aggregates hyperfine JSON results into summary tables"
    - path: "docs/benchmarks.md"
      provides: "Polished benchmark documentation with methodology, results, and analysis"
      min_lines: 100
    - path: "benchmarks/README.md"
      provides: "How-to-run guide for benchmark scripts"
      min_lines: 50
  key_links:
    - from: "benchmarks/scripts/compare_metrics.py"
      to: "benchmarks/results/"
      via: "reads CG JSON output and FTA JSON output for comparison"
      pattern: "json\\.load"
    - from: "docs/benchmarks.md"
      to: "benchmarks/"
      via: "links to benchmarks/ directory for raw data and scripts"
      pattern: "benchmarks/"
    - from: "benchmarks/scripts/summarize_results.py"
      to: "benchmarks/results/"
      via: "reads hyperfine JSON results and produces summary"
      pattern: "json\\.load"
---

<objective>
Create metric accuracy comparison tooling (CG vs FTA) and comprehensive documentation covering methodology, results schema, and analysis. This plan produces the visible deliverables — the benchmark documentation that will be referenced in Phase 11/12 impact analysis.

Purpose: The benchmark data from Plans 01-02 is raw; this plan transforms it into actionable insights. The metric accuracy comparison validates that CG and FTA agree on relative complexity rankings. The documentation provides a reproducible methodology and a baseline for measuring the impact of future features.

Output: compare-metrics.sh, compare_metrics.py, summarize_results.py, docs/benchmarks.md, benchmarks/README.md, README.md updates.
</objective>

<execution_context>
@/home/ben/.claude/get-shit-done/workflows/execute-plan.md
@/home/ben/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10.1-performance-benchmarks-and-fta-comparison/10.1-CONTEXT.md
@.planning/phases/10.1-performance-benchmarks-and-fta-comparison/10.1-RESEARCH.md
@.planning/phases/10.1-performance-benchmarks-and-fta-comparison/10.1-01-SUMMARY.md
@README.md
@docs/benchmarks.md
@publication/npm/README.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create metric accuracy comparison and results summary scripts</name>
  <files>
    benchmarks/scripts/compare-metrics.sh
    benchmarks/scripts/compare_metrics.py
    benchmarks/scripts/summarize_results.py
  </files>
  <action>
    1. **Create `benchmarks/scripts/compare-metrics.sh`**:
       - `set -euo pipefail`
       - Determine PROJECT_ROOT
       - Build CG in ReleaseFast mode
       - Auto-install FTA 3.0.0 into temp dir (same pattern as bench scripts)
       - Accept `--suite` flag (quick/full, default quick)
       - For each project in selected suite:
         a. Run CG: `$CG_BIN --format json --fail-on none $PROJECT_DIR > /tmp/cg-output-$project.json`
         b. Run FTA: `$FTA_BIN --json --exclude-under 0 $PROJECT_DIR > /tmp/fta-output-$project.json 2>/dev/null` (FTA outputs JSON to stdout)
         c. Run comparison: `python3 benchmarks/scripts/compare_metrics.py /tmp/cg-output-$project.json /tmp/fta-output-$project.json $project`
       - Aggregate results: pipe all comparison outputs to final JSON
       - Write results to `$RESULTS_DIR/metric-accuracy.json`
       - Clean up temp files
       - Make executable

    2. **Create `benchmarks/scripts/compare_metrics.py`**:
       - Accept 3 args: CG JSON path, FTA JSON path, project name
       - Parse CG JSON output (schema: `{ files: [{ path, functions: [{ name, cyclomatic, cognitive, halstead_volume, ... }] }] }`)
       - Parse FTA JSON output (schema: FTA outputs an array of objects `[{ file_name, cyclo, fta_score, line_count, ... }]`)
       - For each file that appears in BOTH outputs:
         a. **Cyclomatic**: CG file-level = sum of per-function cyclomatic values; FTA = `cyclo` field
         b. **Halstead volume**: CG file-level = sum of per-function halstead_volume; FTA = `entry["halstead"]["volume"]` (nested inside halstead object, NOT a top-level key)
         c. **Line count**: CG file-level = from file-level structural metrics; FTA = `line_count` field
       - Compute per-file:
         - `diff_pct`: `abs(cg - fta) / max(cg, fta, 1) * 100`
         - `within_tolerance`: diff_pct <= 25.0 for cyclomatic, <= 30.0 for halstead
         - `ranking_match`: whether both tools agree on relative ranking (top 25% files match)
       - Output JSON per project:
         ```json
         {
           "project": "<name>",
           "files_compared": N,
           "files_cg_only": N,
           "files_fta_only": N,
           "cyclomatic": {
             "within_tolerance_pct": 72.5,
             "mean_diff_pct": 18.3,
             "ranking_correlation": 0.85
           },
           "halstead_volume": { ... },
           "line_count": { ... },
           "methodology": {
             "cg_aggregation": "sum of per-function values",
             "fta_granularity": "file-level",
             "cyclomatic_tolerance": 25.0,
             "halstead_tolerance": 30.0,
             "note": "FTA uses SWC parser; CG uses tree-sitter. Different tokenization rules cause expected divergence."
           }
         }
         ```
       - Print human-readable summary to stderr

    3. **Create `benchmarks/scripts/summarize_results.py`**:
       - Accept a results directory path
       - Find all `*-quick.json`, `*-full.json`, `*-stress.json` hyperfine result files
       - For each file, extract: project name, CG mean/stddev, FTA mean/stddev, CG memory, FTA memory
       - Compute speedup ratio: FTA_mean / CG_mean (>1.0 means CG is faster)
       - Compute memory ratio: FTA_memory / CG_memory
       - Output:
         a. Markdown table to stdout (suitable for pasting into docs/benchmarks.md):
            ```
            | Project | CG (ms) | FTA (ms) | Speedup | CG Mem (MB) | FTA Mem (MB) | Mem Ratio |
            |---------|---------|----------|---------|-------------|--------------|-----------|
            | zod     | 44 ± 1  | 61 ± 2   | 1.4x    | 9.2         | 32.4         | 3.5x      |
            ```
         b. JSON summary to file if `--json <path>` flag provided
       - Also find `*-subsystems.json` files and include subsystem breakdown in summary
       - Print overall averages: mean speedup, mean memory ratio across all projects
       - Make executable
  </action>
  <verify>
    - `python3 benchmarks/scripts/compare_metrics.py` with test inputs produces valid JSON comparison
    - `bash benchmarks/scripts/compare-metrics.sh` runs on quick suite and produces metric-accuracy.json
    - `python3 benchmarks/scripts/summarize_results.py benchmarks/results/baseline-YYYY-MM-DD/` produces markdown table
    - All scripts handle missing files gracefully (skip with warning, don't crash)
  </verify>
  <done>
    Metric accuracy comparison tooling works end-to-end: runs both tools, compares per-file metrics with appropriate aggregation and tolerance bands, produces JSON results. Summarize script aggregates hyperfine results into markdown tables with speedup and memory ratios.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive benchmark documentation</name>
  <files>
    benchmarks/README.md
    docs/benchmarks.md
    README.md
    publication/npm/README.md
    publication/npm/packages/darwin-arm64/README.md
    publication/npm/packages/darwin-x64/README.md
    publication/npm/packages/linux-arm64/README.md
    publication/npm/packages/linux-x64/README.md
    publication/npm/packages/windows-x64/README.md
  </files>
  <action>
    1. **Create `benchmarks/README.md`** — practical how-to guide:
       - Prerequisites: Zig 0.15.2+, Node.js (for FTA installation), hyperfine, python3
       - Quick start: 4-command sequence to run full benchmark suite
         ```
         ./benchmarks/scripts/setup.sh --suite quick
         ./benchmarks/scripts/bench-quick.sh
         ./benchmarks/scripts/bench-subsystems.sh
         ./benchmarks/scripts/compare-metrics.sh
         ```
       - Script reference table: name, purpose, output
       - Suite tiers: quick (10 projects, ~5 min), full (76 projects, ~60 min), stress (3 massive, ~30 min)
       - Results directory structure and JSON schema documentation
       - How to interpret results (speedup ratio, memory ratio, subsystem hotspots, metric accuracy tolerance)
       - How to add new benchmark runs (for Phase 11/12 before/after comparison)

    2. **Create `docs/benchmarks.md`** — polished summary page following TanStack-style progressive disclosure:
       - **Opening**: Brief statement about ComplexityGuard's performance characteristics and what was measured
       - **Key Findings** section (upfront, before methodology):
         - Speed comparison: CG vs FTA across project sizes (reference summarize_results.py output)
         - Memory comparison: CG vs FTA peak RSS
         - Subsystem breakdown: where CG spends its time (parsing? analysis? output?)
         - Metric accuracy: how well CG and FTA agree on complexity rankings
       - **Methodology** section:
         - Hardware description: "Benchmarks run on [describe system]" (placeholder for executor to fill in)
         - Tool versions: ComplexityGuard 0.6.0, FTA 3.0.0, hyperfine 1.20.0
         - Statistical approach: 15 runs per project (quick/full), 5 runs (stress), 3 warmup runs
         - Suite composition: quick (10 projects), full (76 projects), stress (3 massive repos)
         - Important caveats:
           - CG is currently single-threaded (Phase 12 will add parallelization)
           - FTA operates at file-level; CG at function-level (more granular = more work per file)
           - FTA uses SWC parser; CG uses tree-sitter (different tokenization rules)
           - `--exclude-under 0` used for FTA to disable minimum-lines filter
           - `--fail-on none` used for CG to prevent threshold violations affecting timing
         - Reproducibility: "Clone this repo and run `benchmarks/scripts/setup.sh && benchmarks/scripts/bench-quick.sh`"
       - **Detailed Results** section:
         - Speed results table (from summarize_results.py)
         - Memory results table
         - Subsystem breakdown table (from bench-subsystems.sh)
         - Metric accuracy summary (from compare-metrics.sh)
       - **Baseline for Future Phases** section:
         - Explain that this data serves as the baseline for Phase 11 (duplication detection) and Phase 12 (parallelization)
         - Schema version 1.0 ensures direct before/after comparison
         - Link to `benchmarks/` directory for raw data and scripts
       - Note: Where specific numbers are referenced, include placeholder markers like `[RESULTS: run summarize_results.py to populate]` that the executor fills in after running the actual benchmarks. The documentation structure and prose must be complete even if specific numbers are TBD.

    3. **Update `README.md`**:
       - Add "Performance" or "Benchmarks" entry to the documentation links section (wherever docs/ pages are listed)
       - Add a brief performance claim in the features section if appropriate (e.g., "Single static binary with fast startup and low memory usage")
       - Link to `docs/benchmarks.md` for details

    4. **Update `publication/npm/README.md`**:
       - Sync with main README changes (add benchmarks link if main README gets one)
       - Keep changes minimal — just add the link to docs/benchmarks.md

    5. **Update all 5 sub-package READMEs** (`publication/npm/packages/*/README.md`):
       - Update: `publication/npm/packages/darwin-arm64/README.md`, `publication/npm/packages/darwin-x64/README.md`, `publication/npm/packages/linux-arm64/README.md`, `publication/npm/packages/linux-x64/README.md`, `publication/npm/packages/windows-x64/README.md`
       - Sync the Links section with any benchmarks link added to the main README
       - Keep changes minimal and consistent across all 5 files

    **Note:** `benchmarks/README.md` references `bench-subsystems.sh` which is created by Plan 02. Since Plan 03 runs in wave 2 parallel with Plan 02, these are forward references that cannot be end-to-end verified until Plan 02 completes. Document the script reference but do not attempt to run or verify `bench-subsystems.sh` from this plan.
  </action>
  <verify>
    - `benchmarks/README.md` exists with quick start instructions and script reference
    - `docs/benchmarks.md` exists with methodology, key findings structure, and baseline section
    - `README.md` links to `docs/benchmarks.md`
    - `publication/npm/README.md` is updated to match
    - All 5 `publication/npm/packages/*/README.md` files are updated with benchmarks link
    - Documentation is clear enough that someone unfamiliar with the project could reproduce the benchmarks
  </verify>
  <done>
    benchmarks/README.md provides practical how-to for running benchmarks. docs/benchmarks.md provides polished summary with methodology, findings, and baseline context for Phase 11/12. README.md, publication/npm/README.md, and all 5 sub-package READMEs link to benchmark documentation.
  </done>
</task>

</tasks>

<verification>
1. `bash benchmarks/scripts/compare-metrics.sh` produces metric-accuracy.json with per-project comparison
2. `python3 benchmarks/scripts/summarize_results.py benchmarks/results/baseline-*/` produces markdown summary table
3. `docs/benchmarks.md` has Methodology, Key Findings, Detailed Results, and Baseline sections
4. `benchmarks/README.md` has Quick Start and Script Reference sections
5. `grep -r "benchmarks" README.md` shows at least one link to docs/benchmarks.md
6. All documentation references correct script names and file paths
</verification>

<success_criteria>
- Metric accuracy comparison validates CG and FTA agree on relative complexity rankings (within documented tolerance)
- Documentation is exhaustive: methodology section is reproducible, results are structured for Phase 11/12 comparison
- Raw data in benchmarks/results/ is committed and serves as historical baseline
- Someone reading docs/benchmarks.md understands CG's performance characteristics without running benchmarks themselves
</success_criteria>

<output>
After completion, create `.planning/phases/10.1-performance-benchmarks-and-fta-comparison/10.1-03-SUMMARY.md`
</output>
