---
phase: 10.1-performance-benchmarks-and-fta-comparison
plan: 03
subsystem: docs
tags: [benchmarks, python, shell-scripts, documentation, fta-comparison, metric-accuracy]

# Dependency graph
requires:
  - phase: 10.1-01
    provides: Benchmark infrastructure (setup.sh, bench-quick/full/stress.sh, baseline results)
provides:
  - compare_metrics.py: per-project CG vs FTA metric comparison with Spearman correlation
  - compare-metrics.sh: orchestrates both tools, produces metric-accuracy.json
  - summarize_results.py: aggregates hyperfine JSON into markdown speed/memory tables
  - docs/benchmarks.md: polished benchmark documentation with methodology and findings
  - benchmarks/README.md: practical how-to guide for running all benchmark scripts
  - metric-accuracy.json: baseline metric accuracy data for 7 quick-suite projects
affects:
  - "Phase 11 (duplication detection) — before/after comparison infrastructure complete"
  - "Phase 12 (parallelization) — baseline documented, comparison workflow defined"

# Tech tracking
tech-stack:
  added: []
  patterns:
    - "Spearman rank correlation for metric accuracy: normalize CG absolute paths to relative via project-name segment stripping, FTA paths already relative"
    - "Speedup = CG_time / FTA_time: ratio > 1.0 means FTA is faster (CG time is the numerator)"
    - "Tolerance band comparison: abs(cg - fta) / max(cg, fta, 1) * 100 <= tolerance"
    - "build.zig: b.addInstallArtifact (not b.installArtifact) for bench binary to avoid blocking default zig build"

key-files:
  created:
    - "benchmarks/scripts/compare-metrics.sh"
    - "benchmarks/scripts/compare_metrics.py"
    - "benchmarks/scripts/summarize_results.py"
    - "benchmarks/README.md"
    - "docs/benchmarks.md"
    - "benchmarks/results/baseline-2026-02-21/metric-accuracy.json"
  modified:
    - "README.md (added Performance Benchmarks docs link and performance feature bullets)"
    - "publication/npm/README.md (added Performance Benchmarks link)"
    - "publication/npm/packages/darwin-arm64/README.md (added benchmarks link)"
    - "publication/npm/packages/darwin-x64/README.md (added benchmarks link)"
    - "publication/npm/packages/linux-arm64/README.md (added benchmarks link)"
    - "publication/npm/packages/linux-x64/README.md (added benchmarks link)"
    - "publication/npm/packages/windows-x64/README.md (added benchmarks link)"
    - "build.zig (changed b.installArtifact to b.addInstallArtifact for bench binary)"

key-decisions:
  - "Path normalization: CG produces absolute paths; FTA produces paths relative to project root. normalize_cg_path strips everything up to the project name segment; FTA paths are used as-is."
  - "Speedup formula: CG_mean / FTA_mean (ratio > 1.0 = FTA is faster). Initial implementation had inverted formula (FTA/CG) which showed CG as faster incorrectly."
  - "build.zig bench artifact: changed from b.installArtifact to b.addInstallArtifact so bench binary is only built when zig build bench is explicitly requested, not blocking default zig build"
  - "Spearman correlation for ranking accuracy: absolute metric values diverge due to parser differences, but ranking correlation (0.55-0.90) tells the more useful story for code review prioritization"

# Metrics
duration: 12min
completed: 2026-02-21
---

# Phase 10.1 Plan 03: Metric Accuracy Comparison and Documentation Summary

**Metric accuracy comparison scripts (compare-metrics.sh, compare_metrics.py, summarize_results.py) and comprehensive benchmark documentation (docs/benchmarks.md, benchmarks/README.md) transforming raw hyperfine data into actionable findings with Spearman ranking correlations and before/after Phase 11/12 comparison infrastructure**

## Performance

- **Duration:** 12 min
- **Started:** 2026-02-21T07:32:06Z
- **Completed:** 2026-02-21T07:44:06Z
- **Tasks:** 2
- **Files modified:** 6 created, 8 modified

## Accomplishments

- Created `compare_metrics.py` with Spearman rank correlation, normalized path handling (CG absolute vs FTA relative), and tolerance band analysis (cyclomatic 25%, halstead 30%, line count 20%)
- Created `summarize_results.py` aggregating hyperfine JSON into markdown speed/memory comparison tables with correct speedup formula (CG time / FTA time)
- Created `compare-metrics.sh` orchestrating both tools across quick/full/stress suites and producing `metric-accuracy.json` via embedded Python aggregation
- Ran compare-metrics.sh on quick suite — generated baseline metric-accuracy.json for 7 projects with real ranking correlations
- Created `benchmarks/README.md` (228 lines) with complete how-to guide, script reference table, JSON schema documentation, and Phase 11/12 before/after workflow
- Created `docs/benchmarks.md` (273 lines) with key findings (1.2-3.8x FTA speed advantage, 1.2-2.2x CG memory advantage), methodology, caveats, and baseline section
- Updated README.md and all 7 npm README files to link to docs/benchmarks.md
- Fixed build.zig bench artifact installation to not block default `zig build`

## Task Commits

Each task was committed atomically:

1. **Task 1: Metric accuracy comparison and results summary scripts** - `f6f1735` (feat)
2. **Task 2: Comprehensive benchmark documentation** - `a2dcd97` (feat)

## Files Created/Modified

- `benchmarks/scripts/compare_metrics.py` - Per-project CG vs FTA metric comparison (Spearman correlation, tolerance bands, normalized path handling)
- `benchmarks/scripts/summarize_results.py` - Aggregate hyperfine JSON into markdown speed/memory tables
- `benchmarks/scripts/compare-metrics.sh` - Orchestrate both tools on quick/full/stress suites, produce metric-accuracy.json
- `benchmarks/results/baseline-2026-02-21/metric-accuracy.json` - Baseline accuracy data: 7 projects, cyclomatic corr 0.54-0.89, halstead corr 0.55-0.90
- `benchmarks/README.md` - 228-line how-to guide: prerequisites, quick start, script reference table, JSON schema, interpretation guide
- `docs/benchmarks.md` - 273-line polished summary: key findings, methodology, caveats, detailed results, Phase 11/12 baseline section
- `README.md` - Added Performance Benchmarks docs link and performance feature bullets
- `publication/npm/README.md` - Added Performance Benchmarks link
- `publication/npm/packages/*/README.md` (5 files) - Added Performance Benchmarks link to all platform READMEs
- `build.zig` - Changed b.installArtifact to b.addInstallArtifact for bench binary

## Baseline Metric Accuracy Data

Ranking correlations (Spearman's rho) from quick-suite run:

| Project | Files | Cyclomatic Corr | Halstead Corr | Line Count Agree |
|---------|-------|----------------|---------------|-----------------|
| got | 68 | 0.560 | 0.890 | 82% |
| dayjs | 283 | 0.797 | 0.702 | 92% |
| zod | 169 | 0.719 | 0.901 | 94% |
| vite | 1,149 | 0.695 | 0.748 | 84% |
| nestjs | 1,624 | 0.548 | 0.732 | 84% |
| webpack | 6,555 | 0.544 | 0.550 | 76% |
| vscode | 5,002 | 0.891 | 0.775 | 74% |

CG and FTA have moderate-to-strong ranking correlation (0.55-0.90) despite low
within-tolerance percentages for absolute values. Parser differences (tree-sitter vs SWC)
cause absolute Halstead values to diverge by 70-85%, but rankings remain correlated.

## Decisions Made

- **Path normalization strategy:** CG produces absolute paths containing the project name as a path segment (e.g., `.../benchmarks/projects/zod/src/types.ts`). The solution strips everything up to and including the project name segment. FTA produces paths already relative to project root (e.g., `src/types.ts`) and needs no normalization.

- **Speedup formula correction:** Initial implementation used `fta_mean / cg_mean` (FTA/CG), which shows ratio < 1 when FTA is faster (confusing). Fixed to `cg_mean / fta_mean` (CG/FTA), so ratio > 1 clearly means "FTA is faster."

- **build.zig bench artifact:** `b.installArtifact(bench_exe)` adds the bench binary to the default install step. When benchmark.zig had compile errors (Plan 02 in-progress), `zig build` blocked compare-metrics.sh. Fix: `b.addInstallArtifact` creates the install step but doesn't wire it into the default install, so `zig build bench` compiles it on demand only.

## Deviations from Plan

### Auto-fixed Issues

**1. [Rule 1 - Bug] Fixed inverted speedup formula in summarize_results.py**
- **Found during:** Task 1 verification (summarize_results.py output showed "CG faster" when FTA was clearly faster)
- **Issue:** `speedup = fta_mean_ms / cg_mean_ms` produces ratio < 1 when FTA is faster, causing labels to read "1.3x CG faster" for a case where FTA was actually 1.26x faster
- **Fix:** Changed formula to `speedup = cg_mean_ms / fta_mean_ms` and updated display logic
- **Files modified:** benchmarks/scripts/summarize_results.py
- **Commit:** f6f1735

**2. [Rule 1 - Bug] Fixed path normalization: CG absolute paths vs FTA relative paths**
- **Found during:** Task 1 verification (compare_metrics.py showed 0 files_compared despite both tools processing same files)
- **Issue:** Original `normalize_path` function applied the same project-name-based stripping to both CG and FTA paths. FTA paths are already relative to project root (e.g., `deno/lib/types.ts`), so stripping fell back to `basename`, losing directory structure and preventing matches.
- **Fix:** Split into `normalize_cg_path` (strips absolute prefix) and FTA path handling (use as-is, just convert backslashes). Result: 169/169 files matched for zod.
- **Files modified:** benchmarks/scripts/compare_metrics.py
- **Commit:** f6f1735

**3. [Rule 3 - Blocking] Fixed build.zig bench binary blocking zig build**
- **Found during:** Task 1 execution (compare-metrics.sh uses `zig build -Doptimize=ReleaseFast` to build CG; this also built bench_exe which was failing)
- **Issue:** `b.installArtifact(bench_exe)` wires bench into the default install step. Plan 02 benchmark.zig was added to git working tree but had compile errors (`import of file outside module path`). This blocked `zig build` entirely, preventing CG from building for comparison.
- **Fix:** Changed `b.installArtifact(bench_exe)` to `b.addInstallArtifact(bench_exe, .{})` which creates the install step without adding it to the default install chain. `zig build bench` still works on demand.
- **Files modified:** build.zig
- **Commit:** f6f1735

---

**Total deviations:** 3 auto-fixed (2 Rule 1 bugs, 1 Rule 3 blocking issue)
**Impact on plan:** All three fixes necessary for correct operation. No scope creep.

## Self-Check: PASSED

All key files verified to exist:
- benchmarks/scripts/compare-metrics.sh - FOUND
- benchmarks/scripts/compare_metrics.py - FOUND
- benchmarks/scripts/summarize_results.py - FOUND
- benchmarks/README.md - FOUND
- docs/benchmarks.md - FOUND
- benchmarks/results/baseline-2026-02-21/metric-accuracy.json - FOUND

All commits verified to exist:
- f6f1735 (Task 1: metric accuracy scripts) - FOUND
- a2dcd97 (Task 2: comprehensive benchmark documentation) - FOUND

---
*Phase: 10.1-performance-benchmarks-and-fta-comparison*
*Completed: 2026-02-21*
