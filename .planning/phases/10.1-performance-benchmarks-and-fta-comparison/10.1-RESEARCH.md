# Phase 10.1: Performance Benchmarks and FTA Comparison - Research

**Researched:** 2026-02-21
**Domain:** Performance benchmarking, tool comparison (Zig binary vs Rust/Node CLI), scientific measurement methodology
**Confidence:** HIGH

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

**Benchmark methodology:**
- Measure both wall-clock time AND peak memory usage
- Hyperfine-style statistical rigor: 10+ runs, warm-up runs, report mean/stddev/min/max
- Dual harness approach: shell scripts + hyperfine for end-to-end binary benchmarks, AND Zig-built benchmark module for subsystem profiling (parsing, analysis, output)
- Manual execution via scripts (no CI automation needed)

**Project selection:**
- Tiered approach: quick suite (~10 representative projects) for fast iteration, full suite (all 76 projects) for release benchmarks
- Projects cloned via shallow clone using git_url and latest_stable_tag from tests/public-projects.json
- Cache after first shallow clone — reuse local copies for subsequent benchmark runs
- Massive projects (vscode, TypeScript) included as stress-test tier — important for validating scalability claims

**FTA comparison scope:**
- Compare speed only for metrics supported by both tools (apples-to-apples on overlapping metrics)
- Also compare metric accuracy: run both tools on same files, compare metric values for overlapping metrics to validate correctness
- Pin a specific FTA version for reproducibility
- Benchmark scripts auto-install pinned FTA version into temp directory (fully self-contained, no pre-install assumption)

**Documentation & output:**
- Raw benchmark data in benchmarks/ directory (JSON format)
- Polished summary in docs/benchmarks.md with key findings and link to benchmarks/ for details
- Benchmark results committed to repo — provides historical record and baseline for Phase 11/12 impact analysis

### Claude's Discretion

- Quick suite project selection (which ~10 projects to include)
- Zig benchmark harness architecture and subsystem boundaries
- Exact hyperfine flags and warm-up configuration
- benchmarks/ directory structure and JSON schema for results
- How to present metric accuracy comparison (diff format, tolerance thresholds)

### Deferred Ideas (OUT OF SCOPE)

None — discussion stayed within phase scope
</user_constraints>

## Summary

Phase 10.1 establishes a rigorous performance baseline for ComplexityGuard by benchmarking it against FTA (Fast TypeScript Analyzer), the primary competing tool. The benchmark suite runs against real-world TypeScript/JavaScript projects from `tests/public-projects.json` across three tiers: quick (10 projects), full (76 projects), and stress-test (massive repos like vscode and TypeScript compiler).

The technical stack is well-defined: hyperfine 1.20.0 (installed at `/home/ben/.cargo/bin/hyperfine`) for end-to-end binary benchmarking with statistical rigor, and Zig's `std.time.Timer` for subsystem profiling within a custom Zig benchmark module. FTA version 3.0.0 (npm package `fta-cli@3.0.0`) is confirmed as the comparison target. Importantly, FTA and ComplexityGuard operate at different granularities — FTA is file-level while CG is function-level — making metric accuracy comparison more nuanced than direct value comparison; the methodology section must document this carefully.

The benchmark JSON schema should be designed upfront to be stable across Phase 11 and Phase 12, so before/after comparisons for duplication detection and parallelization require no schema changes. The `benchmarks/` directory structure should organize results by suite (quick, full, stress) and timestamp, with a `latest/` symlink or explicit naming convention for the current baseline.

**Primary recommendation:** Use hyperfine's `--export-json` with `memory_usage_byte` tracking (confirmed present in hyperfine 1.20.0 JSON output) as the primary data collection mechanism. Auto-install FTA into a temp directory via `npm install fta-cli@3.0.0` as part of each benchmark script run.

## Standard Stack

### Core

| Tool | Version | Purpose | Why Standard |
|------|---------|---------|--------------|
| hyperfine | 1.20.0 | End-to-end binary benchmarking | Statistical rigor, memory tracking, JSON export, widely used for CLI benchmarks |
| fta-cli | 3.0.0 | FTA comparison target | Latest stable, pinned for reproducibility; v3.0.0 includes SWC upgrade with improved operator/operand counting |
| Zig std.time.Timer | (built-in) | Subsystem profiling in Zig benchmark module | Monotonic nanosecond precision, no external dependencies |
| /usr/bin/time -v | (system) | Memory measurement fallback/verification | Captures peak RSS (Maximum resident set size in kbytes) |

### Supporting

| Tool | Version | Purpose | When to Use |
|------|---------|---------|-------------|
| python3 | (system) | Script JSON aggregation, project parsing | Parse public-projects.json, aggregate results across projects |
| node | 25.6.1 (system) | npm install of FTA | Installing pinned fta-cli version into temp directory |
| git | (system) | Shallow clone of benchmark projects | `git clone --branch <tag> --depth 1` for each project |

### Alternatives Considered

| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| hyperfine | GNU time loop in shell | hyperfine provides statistical analysis, outlier detection, memory tracking — no custom stats code needed |
| std.time.Timer | External Zig benchmark lib (zBench, benchmark.zig) | Built-in is sufficient for subsystem timing; no extra dependencies |
| python3 scripts | jq + bash | Python is more readable for JSON aggregation across 76 projects |

**Installation:**
```bash
# hyperfine already installed at:
/home/ben/.cargo/bin/hyperfine

# FTA auto-installed per benchmark run:
npm install fta-cli@3.0.0 --prefix /tmp/fta-bench-$$
```

## Architecture Patterns

### Recommended Project Structure

```
benchmarks/
├── README.md                    # How to run benchmarks, interpret results
├── scripts/
│   ├── setup.sh                 # Clone quick/full suite projects
│   ├── bench-quick.sh           # Run quick suite (10 projects)
│   ├── bench-full.sh            # Run full suite (76 projects)
│   ├── bench-stress.sh          # Run stress-test suite (vscode, TypeScript, etc.)
│   ├── bench-subsystems.sh      # Run Zig subsystem benchmark
│   └── compare-metrics.sh       # Run metric accuracy comparison (CG vs FTA)
├── projects/                    # Shallow-cloned repos (git-ignored, .gitkeep)
│   └── .gitkeep
├── results/
│   ├── baseline-2026-02-21/     # Initial baseline results
│   │   ├── quick-suite.json     # hyperfine JSON for quick suite
│   │   ├── full-suite.json      # hyperfine JSON for full suite
│   │   ├── stress-suite.json    # hyperfine JSON for stress-test suite
│   │   ├── subsystems.json      # Zig subsystem profiling results
│   │   └── metric-accuracy.json # CG vs FTA metric comparison
│   └── README.md                # Schema documentation
└── src/                         # Zig benchmark module source
    └── benchmark.zig            # Subsystem profiling entry point
docs/
└── benchmarks.md                # Polished summary (link to benchmarks/)
```

### Pattern 1: Hyperfine End-to-End Benchmark

**What:** Run both tools against each project, collect statistical timing and memory data per run.
**When to use:** For all end-to-end wall-clock and memory measurements.

```bash
# Source: hyperfine 1.20.0 verified behavior
FTA_BIN="/tmp/fta-bench/node_modules/.bin/fta"
CG_BIN="./zig-out/bin/complexity-guard"

hyperfine \
  --warmup 3 \
  --runs 15 \
  --export-json "benchmarks/results/baseline-2026-02-21/quick-suite.json" \
  --ignore-failure \
  "${CG_BIN} --format json --fail-on none ${PROJECT_DIR}" \
  "${FTA_BIN} --json ${PROJECT_DIR}"
```

Key flags confirmed working:
- `--warmup N`: N warm-up runs (not measured, mitigates cold-start/cache effects)
- `--runs N`: Minimum N benchmark runs
- `--export-json FILE`: Export full results including per-run `times` array and `memory_usage_byte` array
- `--ignore-failure`: Required because CG exits 1 when violations found (use `--fail-on none` flag instead — cleaner)

**Hyperfine JSON output schema (verified):**
```json
{
  "results": [{
    "command": "...",
    "mean": 0.042,     // seconds
    "stddev": 0.001,   // seconds
    "median": 0.042,   // seconds
    "user": 0.039,     // seconds
    "system": 0.003,   // seconds
    "min": 0.041,      // seconds
    "max": 0.044,      // seconds
    "times": [...],    // per-run wall-clock times (seconds)
    "memory_usage_byte": [...], // per-run peak memory (bytes)
    "exit_codes": [...]
  }]
}
```

`memory_usage_byte` is automatically tracked by hyperfine 1.20.0 — no `/usr/bin/time` wrapper needed for the primary measurement. This is a HIGH-confidence finding verified by running actual benchmarks.

### Pattern 2: Zig Subsystem Benchmark Module

**What:** A standalone Zig executable that exercises individual pipeline stages and reports timing statistics.
**When to use:** For understanding which subsystem (parsing, cyclomatic analysis, halstead analysis, output serialization) dominates runtime — critical for Phase 12 parallelization design.

Subsystem boundaries to measure independently:
1. **File discovery + I/O**: `walker.zig` walking directory tree, reading file contents
2. **Parsing**: `parse.zig` calling tree-sitter on each file
3. **Cyclomatic analysis**: `metrics/cyclomatic.zig` per file
4. **Cognitive analysis**: `metrics/cognitive.zig` per file
5. **Halstead + Structural**: `metrics/halstead.zig` + `metrics/structural.zig` per file
6. **Scoring**: `metrics/scoring.zig` per file
7. **Output serialization**: `output/json_output.zig` for all results

```zig
// Pattern using std.time.Timer (Zig 0.15.2 verified)
var timer = try std.time.Timer.start();
// ... run subsystem ...
const elapsed_ns = timer.read();
// timer.reset() to exclude setup from measurement
```

The `std.time.Timer` API:
- `Timer.start()` returns `error{TimerUnsupported}!Timer`
- `timer.read()` returns elapsed nanoseconds as `u64`
- `timer.reset()` resets to zero without allocating

**Important:** The Zig benchmark module should NOT be in `build.zig`'s test step — it's a separate executable (`zig build bench`) that requires benchmark projects to be present.

### Pattern 3: FTA Installation in Benchmark Scripts

**What:** Self-contained FTA installation using npm prefix to isolate from global node_modules.
**When to use:** At the start of every benchmark script run.

```bash
FTA_TEMP=$(mktemp -d /tmp/fta-bench-XXXX)
npm install fta-cli@3.0.0 --prefix "$FTA_TEMP" --quiet 2>/dev/null
FTA_BIN="$FTA_TEMP/node_modules/.bin/fta"
```

Verified: `fta-cli@3.0.0` is current npm package name (NOT `@ftajs/cli` which is an unrelated tool).
Verified: `fta --version` outputs `fta 3.0.0`.

### Pattern 4: Metric Accuracy Comparison

**What:** Run both tools on same files, compare values for overlapping metrics.
**Critical finding:** FTA and ComplexityGuard operate at different granularities:

| Aspect | FTA (fta-cli 3.0.0) | ComplexityGuard 0.6.0 |
|--------|---------------------|----------------------|
| Granularity | File-level | Function-level |
| Cyclomatic | Sum across whole file (uses SWC, different counting rules) | Per-function starting at base 1 |
| Halstead volume | Whole-file token set | Per-function |
| Halstead difficulty | Whole-file | Per-function |
| Halstead bugs | Whole-file | Per-function |
| Line count | Total file lines | Per-function logical lines |

**Comparison strategy:** For metric accuracy, compare CG's per-file aggregate (e.g., sum/max of per-function cyclomatic) against FTA's file-level value. The methodology section in docs/benchmarks.md must explicitly document:
1. FTA operates at file scope; CG operates at function scope
2. For the comparison, CG file-level values are computed as: cyclomatic_max, cyclomatic_sum, halstead_volume_sum
3. Differences in counting rules (FTA uses SWC; CG uses tree-sitter) mean values will differ — document expected divergence ranges, not exact matches

**Tolerance for "match":** Because SWC and tree-sitter tokenize differently, especially for TypeScript type annotations and decorators, expect cyclomatic values within ±20% and halstead within ±30%. The comparison validates that both tools agree on which files are high-complexity vs low-complexity (relative ranking), not absolute values.

### Pattern 5: Quick Suite Project Selection

**What:** Select ~10 representative projects from the 76-project full suite.
**Criteria:** Cover different size ranges, quality tiers, and language mix.

**Recommended quick suite (10 projects):**

| Project | Tier | Size Estimate | Why Selected |
|---------|------|---------------|--------------|
| zod | high | small (~50 files) | Minimal complexity baseline |
| got | high | small-medium | Clean TypeScript, focused scope |
| dayjs | high | small | Small JS library, fast iteration |
| vite | high | medium | Build tool, good mix of TS |
| nestjs | medium | large monorepo | Decorator-heavy, medium quality |
| webpack | low | large | High complexity, JS |
| typeorm | medium | large | Complex query builder |
| rxjs | medium | medium | Reactive patterns, complex |
| effect | high | very large monorepo | Stress test for large TS projects |
| vscode | high | massive | Stress-test tier (100K+ files) |

This gives: 2 small, 3 medium, 3 large, 2 massive — spanning all quality tiers and both languages.

### Anti-Patterns to Avoid

- **Running benchmarks with full build artifacts present**: Always run `zig build -Doptimize=ReleaseFast` (or `ReleaseSafe`) for benchmark builds, not the debug binary. Current `zig-out/bin/complexity-guard` may be a debug build.
- **Benchmarking without warm-up**: Cold-start file system cache effects dominate the first run. Always use `--warmup 3` minimum.
- **Comparing CG debug vs FTA release**: FTA ships pre-compiled Rust; ensure CG is built in release mode for fair comparison.
- **Using FTA's table output for timing**: FTA's default table output adds formatting overhead. Always use `--json` flag for consistent measurement.
- **Ignoring exit codes in analysis**: Use `--fail-on none` in CG to prevent threshold violations from affecting timing (separate speed measurement from quality checking).

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Statistical analysis | Custom mean/stddev calculator | hyperfine | Handles outlier detection, warm-up, reproducibility |
| Memory measurement | /proc polling loop | hyperfine `memory_usage_byte` | Already built in to 1.20.0 JSON export |
| JSON schema for results | Ad-hoc format | Design schema upfront (see below) | Schema stability required for Phase 11/12 before/after diff |
| FTA version management | Git clone + cargo build | `npm install fta-cli@3.0.0 --prefix` | Pre-built binary, fast install, version-pinned |

**Key insight:** Hyperfine already provides `memory_usage_byte` per run in its JSON export — this was verified empirically. No `/usr/bin/time -v` wrapper is needed for memory measurement.

## Common Pitfalls

### Pitfall 1: Build Mode Mismatch

**What goes wrong:** Benchmarking the debug build of ComplexityGuard, which runs ~3-5x slower than release due to no optimizations and safety checks.
**Why it happens:** `zig build` (no optimize flag) defaults to Debug mode.
**How to avoid:** Build with `zig build -Doptimize=ReleaseFast` before benchmarking; verify with `file zig-out/bin/complexity-guard` showing stripped binary.
**Warning signs:** CG timing close to or slower than FTA on small projects.

### Pitfall 2: Filesystem Cache Contamination

**What goes wrong:** First benchmark run reads files from disk; subsequent runs hit OS page cache. Without warm-up, measurements are inconsistent.
**Why it happens:** Modern OS aggressively caches recently read files in RAM.
**How to avoid:** Always `--warmup 3` in hyperfine; for strict cold-cache measurement, add `--prepare 'echo 3 > /proc/sys/vm/drop_caches'` (requires root — only for special testing).
**Warning signs:** Stddev > 10% of mean on fast projects.

### Pitfall 3: CG Exit Code 1 Breaking Hyperfine

**What goes wrong:** CG exits with code 1 when it finds violations. Hyperfine treats non-zero exit as failure.
**Why it happens:** CG's default `--fail-on` is `error`.
**How to avoid:** Always pass `--fail-on none` to CG in benchmark commands. Do NOT use hyperfine's `--ignore-failure` as a workaround (it hides real failures too).
**Warning signs:** Hyperfine "Command terminated with non-zero exit code" error on first warmup.

### Pitfall 4: FTA Missing Files Due to `--exclude-under` Default

**What goes wrong:** FTA skips files with fewer than 6 lines of code by default (`--exclude-under 6`). CG analyzes all files. This makes FTA appear faster because it processes fewer files.
**Why it happens:** FTA's default configuration excludes very small files.
**How to avoid:** Set `--exclude-under 0` in all FTA benchmark commands to disable the minimum-lines filter, ensuring apples-to-apples file count comparison. Document the flag in the methodology section.
**Warning signs:** FTA analyzing fewer files than CG on the same directory.

### Pitfall 5: Metric Granularity Confusion in Accuracy Comparison

**What goes wrong:** Directly comparing FTA's file-level `cyclo=24` to CG's per-function cyclomatic values and concluding the tools disagree.
**Why it happens:** FTA aggregates at file scope; CG reports per-function. CG's sum for `cyclomatic_cases.ts` was 34 (not 24) because of different counting rules (not just aggregation level).
**How to avoid:** Document the comparison methodology explicitly: (1) aggregate CG to file-level, (2) acknowledge different counting rules, (3) compare relative rankings not absolute values, (4) use ±20-30% tolerance bands.
**Warning signs:** Metric accuracy report showing 0% match on any file.

### Pitfall 6: Shallow Clone Depth Insufficient

**What goes wrong:** `--depth 1` fetches only the tip commit but some large repos have large objects even at depth 1.
**Why it happens:** Individual large files (generated code, binaries checked in) inflate clone size.
**How to avoid:** Use `--depth 1 --single-branch --no-tags` to minimize clone size. For vscode and TypeScript repos, expect shallow clones to still be 100-500 MB.
**Warning signs:** Clone taking > 5 minutes on good connection.

## Code Examples

Verified patterns from empirical testing:

### Hyperfine Two-Command Comparison with JSON Export

```bash
#!/usr/bin/env bash
# Source: Verified against hyperfine 1.20.0 behavior
set -euo pipefail

PROJECT="$1"
OUTPUT_JSON="$2"
FTA_BIN="/tmp/fta-bench/node_modules/.bin/fta"
CG_BIN="$(git rev-parse --show-toplevel)/zig-out/bin/complexity-guard"

/home/ben/.cargo/bin/hyperfine \
  --warmup 3 \
  --runs 15 \
  --export-json "$OUTPUT_JSON" \
  "${CG_BIN} --format json --fail-on none ${PROJECT}" \
  "${FTA_BIN} --json --exclude-under 0 ${PROJECT}"
```

### FTA Self-Contained Installation

```bash
#!/usr/bin/env bash
# Source: Verified npm install behavior
FTA_VERSION="3.0.0"
FTA_TEMP=$(mktemp -d /tmp/fta-bench-XXXX)
npm install "fta-cli@${FTA_VERSION}" --prefix "$FTA_TEMP" --quiet 2>/dev/null
FTA_BIN="${FTA_TEMP}/node_modules/.bin/fta"
echo "FTA installed: $($FTA_BIN --version)"  # outputs: fta 3.0.0
```

### Shallow Clone with Cache Check

```bash
#!/usr/bin/env bash
# Source: Verified git behavior
CACHE_DIR="benchmarks/projects"
clone_project() {
  local name="$1" url="$2" tag="$3"
  local dest="${CACHE_DIR}/${name}"
  if [ -d "$dest" ]; then
    echo "Cached: $name"
    return
  fi
  echo "Cloning: $name @ $tag"
  git clone --branch "$tag" --depth 1 --single-branch --no-tags "$url" "$dest"
}
```

### Zig Subsystem Timer Pattern

```zig
// Source: Zig 0.15.2 std.time.Timer verified in codebase
const std = @import("std");

pub fn measureSubsystem(comptime name: []const u8, func: anytype, args: anytype) !u64 {
    var timer = try std.time.Timer.start();
    try @call(.auto, func, args);
    const elapsed_ns = timer.read();
    std.debug.print("{s}: {}ms\n", .{ name, elapsed_ns / 1_000_000 });
    return elapsed_ns;
}
```

### Metric Accuracy Comparison (Python Script Pattern)

```python
#!/usr/bin/env python3
# Compare FTA vs CG per-file metrics with appropriate aggregation
import json, sys, math

def compare_file(fta_file, cg_file):
    """Compare metrics for a single file, tolerating ±25% difference."""
    fta_cyclo = fta_file["cyclo"]
    # CG: aggregate to file level (use sum for total complexity)
    cg_cyclo_sum = sum(fn["cyclomatic"] for fn in cg_file["functions"])

    diff_pct = abs(fta_cyclo - cg_cyclo_sum) / max(fta_cyclo, cg_cyclo_sum, 1) * 100
    return {
        "file": fta_file["file_name"],
        "fta_cyclo": fta_cyclo,
        "cg_cyclo_sum": cg_cyclo_sum,
        "diff_pct": round(diff_pct, 1),
        "within_tolerance": diff_pct <= 25.0
    }
```

### benchmarks/results JSON Schema

```json
{
  "schema_version": "1.0",
  "tool": "complexity-guard",
  "tool_version": "0.6.0",
  "comparison_tool": "fta-cli",
  "comparison_tool_version": "3.0.0",
  "run_date": "2026-02-21",
  "suite": "quick",
  "projects": [
    {
      "name": "zod",
      "git_tag": "v3.24.1",
      "hyperfine_result": { /* ... hyperfine result object ... */ },
      "cg_stats": {
        "files_analyzed": 42,
        "total_functions": 380,
        "wall_clock_mean_ms": 44.2,
        "wall_clock_stddev_ms": 0.8,
        "peak_memory_mb": 9.2
      },
      "fta_stats": {
        "files_analyzed": 38,
        "wall_clock_mean_ms": 61.1,
        "wall_clock_stddev_ms": 2.1,
        "peak_memory_mb": 32.4
      }
    }
  ]
}
```

Note: `schema_version: "1.0"` enables stable before/after comparison in Phase 11/12. After those phases, results will be stored as `schema_version: "1.0"` alongside new runs, allowing direct diff.

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| FTA using SWC v1 | FTA 3.0.0 using upgraded SWC | July 25, 2025 | Different metric values vs older FTA runs — pin to 3.0.0 |
| fta-cli 2.0.x | fta-cli 3.0.0 | July 2025 | v3.0.0 is latest stable; 2.0.2 was previous |
| Manual timing (time command) | hyperfine | Ongoing | Statistical rigor, automatic warm-up, memory tracking |
| hyperfine without memory | hyperfine 1.20.0+ with `memory_usage_byte` | hyperfine 1.20.0 (Nov 2025) | Memory tracking now built-in to JSON export |

**Current versions confirmed:**
- hyperfine: 1.20.0 (installed at `/home/ben/.cargo/bin/hyperfine`, released 2025-11-18)
- fta-cli: 3.0.0 (npm, latest, released July 25, 2025)
- ComplexityGuard: 0.6.0

## Open Questions

1. **Release vs Debug build for CG benchmarks**
   - What we know: `zig-out/bin/complexity-guard` exists but build mode unverified
   - What's unclear: Whether current binary is Debug or ReleaseFast — affects benchmark validity significantly
   - Recommendation: Always rebuild with `zig build -Doptimize=ReleaseFast` at the start of each benchmark script; include build mode in result metadata

2. **File count discrepancy between CG and FTA**
   - What we know: FTA excludes files with < 6 lines by default; CG analyzes all files
   - What's unclear: How large this discrepancy is across the 76 projects in the corpus
   - Recommendation: Use `--exclude-under 0` for FTA in all benchmark runs; document actual file counts in results

3. **vscode shallow clone feasibility**
   - What we know: vscode is listed in public-projects.json at tag `1.96.4`
   - What's unclear: Actual shallow clone size and whether CG can handle the file count without parallelization (Phase 12 feature not yet implemented)
   - Recommendation: Include vscode in stress tier with timeout (e.g., 5-minute limit); document single-threaded limitation in results

4. **Metric accuracy tolerance thresholds**
   - What we know: FTA and CG use different parsers (SWC vs tree-sitter) with different counting rules; empirical test shows cyclomatic_cases.ts: FTA=24, CG_sum=34 (42% difference)
   - What's unclear: Whether this divergence is consistent or file-type-dependent
   - Recommendation: Document observed divergence ranges; frame metric accuracy comparison as "relative ranking agreement" rather than absolute value match

5. **Zig benchmark module build step**
   - What we know: The Zig subsystem benchmark module needs a separate `zig build bench` step
   - What's unclear: Whether to add it to build.zig or keep it as a standalone executable
   - Recommendation: Add as separate `b.step("bench", ...)` in build.zig requiring projects to be cloned first; fail gracefully with clear message if `benchmarks/projects/` is empty

## Sources

### Primary (HIGH confidence)

- Verified empirically: `fta-cli@3.0.0` installed and tested — flags, version, JSON output schema confirmed
- Verified empirically: `hyperfine 1.20.0` installed and tested — JSON schema including `memory_usage_byte` field confirmed by running actual benchmark
- Verified empirically: `/usr/bin/time -v` available on system, outputs "Maximum resident set size (kbytes)"
- Verified empirically: `node 25.6.1`, `npm 11.9.0` available for FTA installation
- Verified empirically: `tests/public-projects.json` contains 76 projects with `git_url` and `latest_stable_tag` fields
- https://ftaproject.dev/docs/configuration — FTA configuration options and CLI flags
- https://github.com/sharkdp/hyperfine — hyperfine documentation

### Secondary (MEDIUM confidence)

- https://ftaproject.dev/docs/scoring — FTA score thresholds (>60=Needs Improvement, 50-60=Could be better, <50=OK)
- https://github.com/sgb-io/fta/releases — FTA v3.0.0 released July 25, 2025 (SWC upgrade, different scores)
- WebSearch results: hyperfine performs at least 10 runs, minimum 3 seconds by default; -w for warmup, -r for run count

### Tertiary (LOW confidence)

- FTA cyclomatic counting rules vs CG (why FTA=24, CG_sum=34 for same file) — not officially documented, only observed empirically on one file

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH — all tools verified installed and tested locally
- Architecture: HIGH — benchmarks directory structure and schema designed from first principles with known constraints
- Pitfalls: HIGH — most pitfalls discovered empirically during research (CG exit code issue, FTA exclude-under behavior)
- Metric accuracy comparison methodology: MEDIUM — based on empirical observation on fixtures, not full corpus

**Research date:** 2026-02-21
**Valid until:** 2026-08-21 (stable tools; FTA and hyperfine release cadence is slow)
