# Phase 10.1: Performance Benchmarks and FTA Comparison - Context

**Gathered:** 2026-02-21
**Status:** Ready for planning

<domain>
## Phase Boundary

Setup extensive performance benchmarks comparing ComplexityGuard against FTA (https://github.com/sgb-io/fta). Test against real-world projects from tests/public-projects.json using scientifically sound methodology. Document results exhaustively as a baseline for impact analysis of upcoming duplication detection (Phase 11) and parallelization (Phase 12) features.

</domain>

<decisions>
## Implementation Decisions

### Benchmark methodology
- Measure both wall-clock time AND peak memory usage
- Hyperfine-style statistical rigor: 10+ runs, warm-up runs, report mean/stddev/min/max
- Dual harness approach: shell scripts + hyperfine for end-to-end binary benchmarks, AND Zig-built benchmark module for subsystem profiling (parsing, analysis, output)
- Manual execution via scripts (no CI automation needed)

### Project selection
- Tiered approach: quick suite (~10 representative projects) for fast iteration, full suite (all 76 projects) for release benchmarks
- Projects cloned via shallow clone using git_url and latest_stable_tag from tests/public-projects.json
- Cache after first shallow clone — reuse local copies for subsequent benchmark runs
- Massive projects (vscode, TypeScript) included as stress-test tier — important for validating scalability claims

### FTA comparison scope
- Compare speed only for metrics supported by both tools (apples-to-apples on overlapping metrics)
- Also compare metric accuracy: run both tools on same files, compare metric values for overlapping metrics to validate correctness
- Pin a specific FTA version for reproducibility
- Benchmark scripts auto-install pinned FTA version into temp directory (fully self-contained, no pre-install assumption)

### Documentation & output
- Raw benchmark data in benchmarks/ directory (JSON format)
- Polished summary in docs/benchmarks.md with key findings and link to benchmarks/ for details
- Benchmark results committed to repo — provides historical record and baseline for Phase 11/12 impact analysis

### Claude's Discretion
- Quick suite project selection (which ~10 projects to include)
- Zig benchmark harness architecture and subsystem boundaries
- Exact hyperfine flags and warm-up configuration
- benchmarks/ directory structure and JSON schema for results
- How to present metric accuracy comparison (diff format, tolerance thresholds)

</decisions>

<specifics>
## Specific Ideas

- Use tests/public-projects.json as the authoritative source for project URLs and tags — shallow clone with `--branch <latest_stable_tag> --depth 1`
- This is explicitly a baseline for measuring the impact of Phase 11 (duplication detection) and Phase 12 (parallelization) — the data must be structured so before/after comparisons are trivial
- Scientific soundness matters: methodology section in docs should be clear enough for someone to reproduce results

</specifics>

<deferred>
## Deferred Ideas

None — discussion stayed within phase scope

</deferred>

---

*Phase: 10.1-performance-benchmarks-and-fta-comparison*
*Context gathered: 2026-02-21*
