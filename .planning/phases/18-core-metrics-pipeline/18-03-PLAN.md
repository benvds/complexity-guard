---
phase: 18-core-metrics-pipeline
plan: 03
type: execute
wave: 3
depends_on: [18-01, 18-02]
files_modified:
  - rust/src/metrics/mod.rs
  - rust/src/metrics/scoring.rs
  - rust/src/metrics/duplication.rs
  - rust/src/types.rs
autonomous: true
requirements: [METR-05, METR-06]

must_haves:
  truths:
    - "Composite health score matches Zig output within 1e-6 float tolerance for all fixture files"
    - "Sigmoid scoring uses the exact formula: 100 / (1 + exp(k * (x - x0)))"
    - "Duplication clone groups match Zig output for duplication_cases.ts"
    - "Rabin-Karp hash uses HASH_BASE=37 and MAX_BUCKET_SIZE=1000 matching Zig constants"
    - "Type 2 clone detection normalizes identifiers to 'V' sentinel"
    - "Tokenizer skips comments, semicolons, commas, and hash_bang_line"
    - "analyze_file() wires all 6 metric families into a single per-file result with embedded token sequence"
  artifacts:
    - path: "rust/src/metrics/scoring.rs"
      provides: "Sigmoid scoring and health score computation"
      exports: ["sigmoid_score", "compute_steepness", "compute_function_score", "compute_file_score"]
    - path: "rust/src/metrics/duplication.rs"
      provides: "Rabin-Karp duplication detection with Type 1 and Type 2 clones"
      exports: ["tokenize_tree", "detect_duplication"]
    - path: "rust/src/metrics/mod.rs"
      provides: "analyze_file entry point assembling all metrics"
      exports: ["analyze_file"]
  key_links:
    - from: "rust/src/metrics/mod.rs"
      to: "rust/src/metrics/scoring.rs"
      via: "compute_function_score called per function in analyze_file"
      pattern: "scoring::compute_function_score"
    - from: "rust/src/metrics/mod.rs"
      to: "rust/src/metrics/duplication.rs"
      via: "tokenize_tree called during analyze_file before tree is dropped"
      pattern: "duplication::tokenize_tree"
    - from: "rust/src/metrics/mod.rs"
      to: "rust/src/metrics/cyclomatic.rs"
      via: "cyclomatic::analyze_functions called from analyze_file"
      pattern: "cyclomatic::analyze_functions"
---

<objective>
Complete the metrics pipeline with scoring, duplication detection, and the analyze_file() entry point that wires all six metric families together into a single per-file result.

Purpose: Delivers the final three components: health scoring (sigmoid normalization), duplication detection (Rabin-Karp with Type 1/2 clones), and the critical analyze_file() function that assembles all metrics in a single pass — embedding tokenization to avoid the Zig re-parse overhead.

Output: Complete `rust/src/metrics/` module where `analyze_file()` produces a `FileAnalysisResult` containing all metrics and token sequences, validated against Zig binary output.
</objective>

<execution_context>
@/Users/benvds/.claude/get-shit-done/workflows/execute-plan.md
@/Users/benvds/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/18-core-metrics-pipeline/18-RESEARCH.md
@.planning/phases/18-core-metrics-pipeline/18-01-SUMMARY.md
@.planning/phases/18-core-metrics-pipeline/18-02-SUMMARY.md
@rust/src/types.rs
@rust/src/metrics/mod.rs
@src/metrics/scoring.zig
@src/metrics/duplication.zig
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement scoring module with sigmoid normalization</name>
  <files>
    rust/src/metrics/scoring.rs
    rust/src/types.rs
    rust/src/metrics/mod.rs
  </files>
  <action>
**Add to rust/src/types.rs:**
- `ScoringWeights` struct: `cyclomatic: f64`, `cognitive: f64`, `halstead: f64`, `structural: f64`, `duplication: f64` (with `Default` matching: 0.20, 0.30, 0.15, 0.15, 0.20)
- `ScoringThresholds` struct with explicit warning/error `f64` pairs for each of the six metrics used in function scoring (matching Zig's `MetricThresholds`):
  - `cyclomatic_warning: f64`, `cyclomatic_error: f64` (defaults: 10.0, 20.0)
  - `cognitive_warning: f64`, `cognitive_error: f64` (defaults: 15.0, 25.0)
  - `halstead_warning: f64`, `halstead_error: f64` (defaults: 500.0, 1000.0)
  - `function_length_warning: f64`, `function_length_error: f64` (defaults: 30.0, 60.0)
  - `params_count_warning: f64`, `params_count_error: f64` (defaults: 4.0, 8.0)
  - `nesting_depth_warning: f64`, `nesting_depth_error: f64` (defaults: 3.0, 6.0)
  - Implement `Default` with the above defaults

**Add to rust/src/metrics/mod.rs:** `pub mod scoring;`

**Create rust/src/metrics/scoring.rs** porting from `src/metrics/scoring.zig`.

**Public functions:**

1. `pub fn sigmoid_score(x: f64, x0: f64, k: f64) -> f64`
   - Formula: `100.0 / (1.0 + (k * (x - x0)).exp())`

2. `pub fn compute_steepness(warning: f64, error: f64) -> f64`
   - If `error <= warning`, return 1.0
   - Otherwise: `4_f64.ln() / (error - warning)`

3. `pub fn compute_function_score(cyclomatic: u32, cognitive: u32, halstead_volume: f64, function_length: u32, params_count: u32, nesting_depth: u32, weights: &ScoringWeights, thresholds: &ScoringThresholds) -> f64`
   - Compute sigmoid score for each metric using its threshold pair
   - Compute steepness from each metric's warning/error thresholds
   - The midpoint (x0) for sigmoid is the average of warning and error thresholds
   - Weight normalization: sum active weights, divide each by sum (handles 4-metric mode when duplication disabled)
   - If all weights are zero, fall back to equal weights
   - Return weighted average of individual sigmoid scores

4. `pub fn compute_file_score(function_scores: &[f64]) -> f64`
   - Arithmetic mean of function scores; if empty, return 100.0

5. `pub fn compute_project_score(file_scores: &[f64], function_counts: &[u32]) -> f64`
   - Function-count-weighted average; if total functions == 0, return 100.0

**Tests:**
- Test `sigmoid_score(0.0, 10.0, 0.5)` and verify against hand-calculated expected value
- Test `compute_steepness(10.0, 20.0)` → `ln(4) / 10.0`
- Test `compute_file_score` with known values
- Test `compute_project_score` with weighted scenario
- Test weight normalization when duplication weight is 0 (4-metric mode)
- Test edge case: all weights zero → equal distribution
  </action>
  <verify>
    <automated>cd /Users/benvds/code/complexity-guard/rust && cargo test metrics::scoring -- --nocapture 2>&1</automated>
  </verify>
  <done>Scoring module produces correct sigmoid values, steepness derivation, and weighted score aggregation. All edge cases (empty functions, zero weights) handled.</done>
</task>

<task type="auto">
  <name>Task 2: Implement duplication detection with Rabin-Karp hashing</name>
  <files>
    rust/src/metrics/duplication.rs
    rust/src/types.rs
    rust/src/metrics/mod.rs
  </files>
  <action>
**Add to rust/src/types.rs:**
- `Token` struct: `kind: String`, `start_byte: usize`, `end_byte: usize`, `file_index: usize` (with `#[derive(Debug, Clone)]`)
- `CloneGroup` struct: `instances: Vec<CloneInstance>`, `token_count: u32`
- `CloneInstance` struct: `file_index: usize`, `start_token: usize`, `end_token: usize`, `start_line: usize`, `end_line: usize`
- `DuplicationResult` struct: `clone_groups: Vec<CloneGroup>`, `total_tokens: usize`, `cloned_tokens: usize`, `duplication_percentage: f64`
- `DuplicationConfig` struct: `min_tokens: u32` (default 25), `enabled: bool` (default true)

**Add to rust/src/metrics/mod.rs:** `pub mod duplication;`

**Create rust/src/metrics/duplication.rs** porting from `src/metrics/duplication.zig`.

**Constants (must match Zig exactly):**
```rust
const HASH_BASE: u64 = 37;
const MAX_BUCKET_SIZE: usize = 1000;
```

**Public functions:**

1. `pub fn tokenize_tree(root: tree_sitter::Node, source: &[u8], file_index: usize) -> Vec<Token>`
   - DFS traversal collecting leaf nodes
   - Skip nodes matching `is_skipped_kind()`: `comment`, `line_comment`, `block_comment`, `;`, `,`, `hash_bang_line`
   - Normalize identifier kinds to `"V"` via `normalize_kind()`:
     - `identifier`, `property_identifier`, `shorthand_property_identifier`, `shorthand_property_identifier_pattern` → `"V"`
     - All other kinds → keep as-is (use `node.kind().to_string()`)
   - Store `Token { kind, start_byte, end_byte, file_index }`

2. `pub fn detect_duplication(file_tokens: &[Vec<Token>], config: &DuplicationConfig) -> DuplicationResult`
   - For each file's token sequence, compute rolling hash windows of size `config.min_tokens`
   - Build hash index: `FxHashMap<u64, Vec<(file_index, token_position)>>`
   - Enforce `MAX_BUCKET_SIZE` — if a bucket exceeds 1000 entries, skip it (common code patterns)
   - Form clone groups from hash collisions with content verification (compare actual token kind sequences)
   - Merge overlapping clone instances within the same file
   - Count total cloned tokens using interval merging (sort-and-sweep)
   - Compute `duplication_percentage = cloned_tokens / total_tokens * 100`

**Rolling hash implementation (Rabin-Karp):**
- `token_hash(kind: &str) -> u64`: hash each byte with `h = h.wrapping_mul(HASH_BASE).wrapping_add(byte as u64)`
- `RollingHasher` struct with `hash: u64` and `base_pow: u64` (= HASH_BASE^(window-1))
- `new()`: compute initial hash for first window
- `roll(remove_token, add_token)`: slide window by one token

**Tests:**
- Test `tokenize_tree` on `tests/fixtures/typescript/duplication_cases.ts` — verify token count and that identifiers are normalized to `"V"`. The file contains 5 functions:
  ```
  processUserData:   line_count=4, params_count=1
  processItemData:   line_count=4, params_count=1
  validateEmail:     line_count=4, params_count=1
  validatePhone:     line_count=4, params_count=1
  uniqueHelper:      line_count=4, params_count=0
  ```
  Note: Zig output shows `"duplication": null` for single-file analysis (duplication requires multi-file). Test that tokenization produces tokens where all identifier/property_identifier kinds are `"V"`.
- Test `detect_duplication` across multiple files by duplicating `duplication_cases.ts` tokens — processUserData and processItemData have nearly identical structure (Type 2 clones after normalization).
- Test that `MAX_BUCKET_SIZE` limiting prevents false positives on common patterns.
- Test `token_hash` for deterministic output.
- Test `is_skipped_kind` correctly filters comments and punctuation.
  </action>
  <verify>
    <automated>cd /Users/benvds/code/complexity-guard/rust && cargo test metrics::duplication -- --nocapture 2>&1</automated>
  </verify>
  <done>Duplication detection finds clone groups matching Zig output. Rabin-Karp hash uses correct constants. Type 2 normalization works. Token skipping is correct.</done>
</task>

<task type="auto">
  <name>Task 3: Wire analyze_file() entry point and add integration tests</name>
  <files>
    rust/src/metrics/mod.rs
    rust/src/types.rs
  </files>
  <action>
**Add to rust/src/types.rs:**
- `FunctionAnalysisResult` struct combining all per-function metrics:
  - `name: String`, `start_line: usize`, `end_line: usize`, `start_col: usize`
  - `cyclomatic: u32`, `cognitive: u32`
  - `halstead_volume: f64`, `halstead_difficulty: f64`, `halstead_effort: f64`, `halstead_time: f64`, `halstead_bugs: f64`
  - `function_length: u32`, `params_count: u32`, `nesting_depth: u32`
  - `health_score: f64`
  - Derive `Debug, Clone, serde::Serialize`

- `FileAnalysisResult` struct:
  - `path: std::path::PathBuf`
  - `functions: Vec<FunctionAnalysisResult>`
  - `tokens: Vec<Token>` (for subsequent duplication pass)
  - `file_score: f64`
  - `file_length: u32`, `export_count: u32`
  - `error: bool`
  - Derive `Debug, Clone, serde::Serialize`

- `AnalysisConfig` struct combining all metric configs:
  - `cyclomatic: CyclomaticConfig`
  - `cognitive: CognitiveConfig`
  - `scoring_weights: ScoringWeights`
  - `scoring_thresholds: ScoringThresholds`
  - `duplication: DuplicationConfig`
  - Implement `Default`

**Implement in rust/src/metrics/mod.rs:**

`pub fn analyze_file(path: &std::path::Path, config: &AnalysisConfig) -> Result<FileAnalysisResult, crate::types::ParseError>`

This function:
1. Calls `crate::parser::select_language(path)?` and reads file
2. Parses with tree-sitter
3. Calls all metric analyzers on the same root node:
   - `cyclomatic::analyze_functions(root, &source, &config.cyclomatic)`
   - `cognitive::analyze_functions(root, &source)`
   - `halstead::analyze_functions(root, &source)`
   - `structural::analyze_functions(root, &source)`
   - `structural::analyze_file(&source, root)`
4. Calls `duplication::tokenize_tree(root, &source, 0)` BEFORE tree is dropped (critical: avoids re-parse)
5. Merges per-function results by index (all walkers discover functions in same DFS order — assert equal lengths)
6. Computes per-function health scores via `scoring::compute_function_score()`
7. Computes file score via `scoring::compute_file_score()`
8. Returns `FileAnalysisResult` with all data

**Add integration tests** in `rust/src/metrics/mod.rs` (in `#[cfg(test)] mod tests`):
- Test `analyze_file` on `tests/fixtures/typescript/simple_function.ts` — expected (from Zig binary output):
  ```
  greet: cyclomatic=1, cognitive=0, halstead_volume=2.0, halstead_difficulty=0.5,
         halstead_effort=1.0, halstead_bugs=0.0006666666666666666,
         line_count=1, params_count=1, nesting_depth=0, health_score=82.71258735483063
  file_length=2, export_count=1
  ```
- Test `analyze_file` on `tests/fixtures/typescript/cyclomatic_cases.ts` — verify cyclomatic values match expected values from 18-01 plan (11 functions, e.g. baseline=1, loopWithConditions=5, etc.)
- Test `analyze_file` on `tests/fixtures/typescript/cognitive_cases.ts` — verify cognitive values match expected values from 18-02 plan (16 functions, e.g. baseline=0, deeplyNested=10, etc.)
- Test `analyze_file` on `tests/fixtures/typescript/halstead_cases.ts` — verify halstead values match expected from 18-02 plan (within 1e-6 tolerance)
- Test `analyze_file` on `tests/fixtures/typescript/structural_cases.ts` — verify structural values match expected from 18-01 plan (9 functions)
- Test that `result.tokens` is non-empty (tokenization embedded in single pass)
- Test that `result.file_score` is between 0.0 and 100.0
  </action>
  <verify>
    <automated>cd /Users/benvds/code/complexity-guard/rust && cargo test metrics:: -- --nocapture 2>&1</automated>
  </verify>
  <done>analyze_file() produces complete FileAnalysisResult with all 6 metric families. All integration tests pass with values matching Zig output. Token sequence embedded without re-parse.</done>
</task>

</tasks>

<verification>
```bash
# All metrics tests pass (all 6 modules + integration)
cd /Users/benvds/code/complexity-guard/rust && cargo test metrics:: -q

# All existing tests still pass
cd /Users/benvds/code/complexity-guard/rust && cargo test -q

# No compiler warnings
cd /Users/benvds/code/complexity-guard/rust && cargo clippy -- -D warnings

# Verify all metric modules exist
ls rust/src/metrics/{mod,cyclomatic,cognitive,halstead,structural,scoring,duplication}.rs
```
</verification>

<success_criteria>
1. `analyze_file()` produces a `FileAnalysisResult` with all metric fields populated for every fixture file
2. Scoring sigmoid formula verified: `sigmoid_score(0, x0, k)` produces expected values
3. Duplication clone groups match Zig output for duplication fixture files
4. Token sequence is embedded in `FileAnalysisResult` without re-parsing the file
5. All six metric modules compile and pass tests together: `cargo test metrics::` succeeds
6. Health score values are between 0.0 and 100.0 for all functions
7. `cargo test -q` passes with zero failures across all test modules
</success_criteria>

<output>
After completion, create `.planning/phases/18-core-metrics-pipeline/18-03-SUMMARY.md`
</output>
