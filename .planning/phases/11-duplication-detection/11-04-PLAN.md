---
phase: 11-duplication-detection
plan: 04
type: execute
wave: 3
depends_on:
  - "11-02"
files_modified:
  - docs/duplication-detection.md
  - docs/getting-started.md
  - docs/cli-reference.md
  - docs/examples.md
  - docs/benchmarks.md
  - README.md
  - publication/npm/README.md
  - publication/npm/packages/complexity-guard-darwin-arm64/README.md
  - publication/npm/packages/complexity-guard-darwin-x64/README.md
  - publication/npm/packages/complexity-guard-linux-arm64/README.md
  - publication/npm/packages/complexity-guard-linux-x64/README.md
  - publication/npm/packages/complexity-guard-win32-x64/README.md
  - benchmarks/scripts/bench-duplication.sh
autonomous: true
requirements:
  - DUP-01
  - DUP-02
  - DUP-03
  - DUP-04
  - DUP-05
  - DUP-06
  - DUP-07

must_haves:
  truths:
    - "Dedicated duplication-detection.md explains the algorithm, configuration, and output formats"
    - "getting-started.md mentions duplication as a metric family with opt-in instructions"
    - "cli-reference.md documents --duplication flag, --metrics duplication, and threshold config"
    - "examples.md includes duplication usage example"
    - "README.md lists duplication as a feature with link to docs"
    - "Publication READMEs synced with main README"
    - "Benchmark script measures duplication analysis performance"
    - "benchmarks.md updated with duplication performance results"
  artifacts:
    - path: "docs/duplication-detection.md"
      provides: "Comprehensive duplication detection documentation"
      min_lines: 80
    - path: "benchmarks/scripts/bench-duplication.sh"
      provides: "Reproducible benchmark script for duplication performance"
      min_lines: 20
  key_links:
    - from: "README.md"
      to: "docs/duplication-detection.md"
      via: "documentation link"
      pattern: "duplication-detection"
    - from: "docs/getting-started.md"
      to: "docs/duplication-detection.md"
      via: "cross-reference link"
      pattern: "duplication-detection"
---

<objective>
Document duplication detection and benchmark its performance impact.

Purpose: Create comprehensive documentation following the project's TanStack-style progressive disclosure pattern. Update all existing docs to reference duplication as a metric family. Create a reproducible benchmark script measuring duplication analysis overhead. Update publication READMEs to stay in sync per CLAUDE.md GSD workflow rules.

Output: New docs/duplication-detection.md page, updated existing docs, benchmark script, performance results.
</objective>

<execution_context>
@/home/ben/.claude/get-shit-done/workflows/execute-plan.md
@/home/ben/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-duplication-detection/11-CONTEXT.md
@.planning/phases/11-duplication-detection/11-RESEARCH.md
@.planning/phases/11-duplication-detection/11-01-SUMMARY.md
@.planning/phases/11-duplication-detection/11-02-SUMMARY.md
@docs/cognitive-complexity.md (reference for doc page style)
@docs/getting-started.md
@docs/cli-reference.md
@docs/examples.md
@docs/benchmarks.md
@README.md
@publication/npm/README.md
@benchmarks/scripts/bench-quick.sh (reference for benchmark script style)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create duplication-detection.md and update existing docs pages</name>
  <files>docs/duplication-detection.md, docs/getting-started.md, docs/cli-reference.md, docs/examples.md, README.md, publication/npm/README.md, publication/npm/packages/complexity-guard-darwin-arm64/README.md, publication/npm/packages/complexity-guard-darwin-x64/README.md, publication/npm/packages/complexity-guard-linux-arm64/README.md, publication/npm/packages/complexity-guard-linux-x64/README.md, publication/npm/packages/complexity-guard-win32-x64/README.md</files>
  <action>
**docs/duplication-detection.md** — New dedicated page following project doc style (see cognitive-complexity.md for pattern):

Structure:
1. **Introduction** — What is code duplication detection? Why it matters. Credit Rabin-Karp approach.
2. **Quick Start** — Enable with `--duplication` flag, show example output
3. **How It Works** — Algorithm overview:
   - Tokenization: AST leaf nodes, comments/whitespace stripped, identifiers normalized
   - Rolling hash: Rabin-Karp sliding window (default 25 tokens)
   - Cross-file indexing: Hash-to-location mapping
   - Verification: Token-by-token confirmation (no false positives from hash collisions)
   - Interval merging: Overlapping clones merged into maximal spans
4. **Clone Types** — Type 1 (exact) and Type 2 (identifier-normalized) detection
5. **Enabling Duplication Detection** — Three equivalent paths:
   - `--duplication` flag
   - `--metrics duplication`
   - Config file: `"analysis": { "duplication_enabled": true }`
6. **Thresholds** — Default values table (file warning 15%, file error 25%, project warning 5%, project error 10%), how to configure
7. **Output Formats** — How duplication appears in each format (console, JSON, SARIF, HTML) with examples
8. **Health Score Impact** — How duplication percentage feeds into the composite score when enabled (0.20 weight, 5-metric normalization)
9. **Performance** — Link to benchmarks, note on opt-in design
10. **Configuration Reference** — Full JSON config example with all duplication fields

Use friendly, thorough tone (TanStack/Astro style per project conventions).

**docs/getting-started.md** — Update:
- Add "Duplication Detection" to the metric families overview section
- Note it's opt-in, link to duplication-detection.md
- Update the "5 metric families" count if currently showing 4

**docs/cli-reference.md** — Update:
- Add `--duplication` flag to the options table
- Add `duplication` to the `--metrics` accepted values list
- Add DuplicationThresholds to the config file schema section
- Update JSON output schema example to include duplication object
- Update SARIF rules list to include duplication rule

**docs/examples.md** — Update:
- Add "Duplication Detection" recipe section:
  - Basic: `complexity-guard --duplication src/`
  - With custom thresholds via config
  - JSON output with duplication data
  - Example console output showing clone groups

**README.md** — Update:
- Add "Duplication Detection" to the features list (with brief description)
- Add `--duplication` to the CLI flags quick reference
- Link to docs/duplication-detection.md
- Update "5 metric families" count if applicable

**Publication READMEs** — Sync with main README:
- publication/npm/README.md
- All 5 platform package READMEs (darwin-arm64, darwin-x64, linux-arm64, linux-x64, win32-x64)
- Update the "What ComplexityGuard Measures" section to include duplication

Commit: `docs(11-04): add duplication detection documentation and update existing docs`
  </action>
  <verify>All doc files exist and reference duplication. Links in docs are valid (relative paths). README mentions duplication in features. Publication READMEs synced.</verify>
  <done>Comprehensive duplication-detection.md page exists. All existing docs updated with duplication references. README lists duplication as a feature. Publication READMEs synced.</done>
</task>

<task type="auto">
  <name>Task 2: Create duplication benchmark script and document performance results</name>
  <files>benchmarks/scripts/bench-duplication.sh, docs/benchmarks.md</files>
  <action>
**benchmarks/scripts/bench-duplication.sh** — Per CONTEXT.md locked decision:
"Reproducible benchmark script: `benchmarks/scripts/bench-duplication.sh`"

Create a benchmark script following bench-quick.sh patterns:
```bash
#!/usr/bin/env bash
set -euo pipefail

# Benchmark duplication detection overhead
# Measures wall time with and without --duplication flag
# Uses the quick benchmark suite projects

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
PROJECTS_DIR="${SCRIPT_DIR}/../projects"
CG_BIN="${SCRIPT_DIR}/../../zig-out/bin/complexity-guard"

# Verify binary exists
if [[ ! -f "$CG_BIN" ]]; then
    echo "Error: complexity-guard binary not found. Run 'zig build' first."
    exit 1
fi

# Verify hyperfine
if ! command -v hyperfine &>/dev/null; then
    echo "Error: hyperfine not found. Install with: cargo install hyperfine"
    exit 1
fi

# Quick suite projects (subset for duplication benchmark)
PROJECTS=("zod" "got" "dayjs")

echo "=== Duplication Detection Benchmark ==="
echo "Binary: $CG_BIN"
echo "Date: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
echo ""

for project in "${PROJECTS[@]}"; do
    project_dir="$PROJECTS_DIR/$project"
    if [[ ! -d "$project_dir" ]]; then
        echo "SKIP: $project (not cloned — run setup.sh first)"
        continue
    fi

    echo "--- $project ---"

    # Count files
    file_count=$(find "$project_dir" -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" 2>/dev/null | wc -l)
    echo "Files: $file_count"

    # Benchmark without duplication
    echo "Without --duplication:"
    hyperfine --warmup 2 --min-runs 5 --ignore-failure \
        "$CG_BIN --fail-on none $project_dir" \
        --export-json "/tmp/bench-dup-${project}-without.json" 2>&1 | grep -E "Time|Range"

    # Benchmark with duplication
    echo "With --duplication:"
    hyperfine --warmup 2 --min-runs 5 --ignore-failure \
        "$CG_BIN --fail-on none --duplication $project_dir" \
        --export-json "/tmp/bench-dup-${project}-with.json" 2>&1 | grep -E "Time|Range"

    echo ""
done

echo "=== Results saved to /tmp/bench-dup-*.json ==="
```

Make the script executable: `chmod +x benchmarks/scripts/bench-duplication.sh`

**docs/benchmarks.md** — Update:
- Add "Duplication Detection Performance" section
- Document expected overhead: duplication adds a cross-file analysis pass
- Include table structure for results:
  | Project | Files | Without Duplication | With Duplication | Overhead |
  |---------|-------|---------------------|------------------|----------|
  | zod     | N     | Xms                 | Yms              | +Z%      |
- Note that duplication is disabled by default to preserve baseline performance
- Reference bench-duplication.sh for reproducible measurement
- Mark actual numbers as TBD (to be filled after running benchmarks) — the executor should run the benchmark and fill in real numbers if the projects are cloned, otherwise leave as placeholder

Commit: `feat(11-04): add duplication benchmark script and update benchmarks documentation`
  </action>
  <verify>`bash benchmarks/scripts/bench-duplication.sh` runs (or shows "project not cloned" messages if projects aren't set up). docs/benchmarks.md includes duplication section.</verify>
  <done>Benchmark script exists at benchmarks/scripts/bench-duplication.sh. docs/benchmarks.md updated with duplication performance section. Script is executable and follows project benchmark patterns.</done>
</task>

</tasks>

<verification>
1. `docs/duplication-detection.md` exists with algorithm explanation, configuration, and examples
2. `docs/getting-started.md` references duplication as a metric family
3. `docs/cli-reference.md` documents --duplication flag and config schema
4. `docs/examples.md` includes duplication recipe
5. `README.md` lists duplication in features
6. All publication READMEs synced
7. `benchmarks/scripts/bench-duplication.sh` is executable and follows bench-quick.sh patterns
8. `docs/benchmarks.md` has duplication performance section
</verification>

<success_criteria>
- Comprehensive docs/duplication-detection.md page following project style
- All existing docs updated to reference duplication
- README and publication READMEs synced
- Benchmark script exists and is reproducible
- Performance section in benchmarks.md
</success_criteria>

<output>
After completion, create `.planning/phases/11-duplication-detection/11-04-SUMMARY.md`
</output>
