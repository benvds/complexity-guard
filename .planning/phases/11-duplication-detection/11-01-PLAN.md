---
phase: 11-duplication-detection
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/metrics/duplication.zig
  - tests/fixtures/typescript/duplication_cases.ts
  - src/main.zig
  - src/lib.zig
autonomous: true
requirements:
  - DUP-01
  - DUP-02
  - DUP-03
  - DUP-04
  - DUP-05

must_haves:
  truths:
    - "Duplication module tokenizes a TypeScript AST into normalized token sequences stripping comments and whitespace"
    - "Identifiers are normalized to sentinel 'V' for Type 2 clone detection"
    - "Rolling hash correctly computes and slides over token windows of configurable size"
    - "Cross-file hash index maps hash values to token window locations across multiple files"
    - "Hash collision verification confirms token-by-token match before forming clone groups"
    - "Overlapping clone intervals are merged into maximal spans per file"
  artifacts:
    - path: "src/metrics/duplication.zig"
      provides: "Core duplication detection algorithm"
      exports: ["Token", "TokenWindow", "CloneGroup", "CloneLocation", "FileDuplicationResult", "DuplicationResult", "DuplicationConfig", "tokenizeTree", "detectDuplication"]
      min_lines: 200
    - path: "tests/fixtures/typescript/duplication_cases.ts"
      provides: "Test fixture with known duplicate code blocks"
      min_lines: 30
  key_links:
    - from: "src/metrics/duplication.zig"
      to: "src/parser/tree_sitter.zig"
      via: "Node API for AST leaf traversal"
      pattern: "tree_sitter\\.Node"
    - from: "src/main.zig"
      to: "src/metrics/duplication.zig"
      via: "test import for discovery"
      pattern: "@import.*duplication"
---

<objective>
Implement the core duplication detection algorithm as a TDD module.

Purpose: Build the algorithmic foundation for cross-file clone detection using Rabin-Karp rolling hash. This module handles tokenization (DUP-01), identifier normalization (DUP-02), rolling hash computation (DUP-03), cross-file hash index building and verification (DUP-04), and overlapping match merging (DUP-05). The module exposes a single `detectDuplication` entry point that takes tokenized file data and returns clone groups with locations.

Output: `src/metrics/duplication.zig` with comprehensive tests, `tests/fixtures/typescript/duplication_cases.ts` with annotated duplicate code blocks.
</objective>

<execution_context>
@/home/ben/.claude/get-shit-done/workflows/execute-plan.md
@/home/ben/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-duplication-detection/11-RESEARCH.md
@src/metrics/halstead.zig (tokenization pattern reference — leaf node traversal, isTypeOnlyNode)
@src/parser/tree_sitter.zig (Node API: childCount, child, nodeType, startPoint, startByte)
@src/metrics/cyclomatic.zig (ThresholdResult struct pattern, validateThreshold, validateThresholdF64)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create duplication_cases.ts fixture and write failing tests for tokenization, hashing, and clone detection</name>
  <files>tests/fixtures/typescript/duplication_cases.ts, src/metrics/duplication.zig, src/main.zig, src/lib.zig</files>
  <action>
**Fixture file** (`tests/fixtures/typescript/duplication_cases.ts`):
Create a TypeScript file with:
- Two identical functions with different names (exact Type 1 clone, ~30 tokens each)
- Two functions with identical structure but different variable names (Type 2 clone, ~30 tokens each)
- A unique function with no duplicate anywhere (control case)
- Short inline comments documenting which blocks are expected clones

Example structure:
```typescript
// Clone Group A: identical logic, different names (Type 1 after normalization)
function processUserData(input: string): string {
  const result = input.trim().toLowerCase();
  if (result.length === 0) {
    return "empty";
  }
  return result.split(",").join(";");
}

function processItemData(input: string): string {
  const result = input.trim().toLowerCase();
  if (result.length === 0) {
    return "empty";
  }
  return result.split(",").join(";");
}

// Clone Group B: same structure, different identifiers (Type 2)
function validateEmail(email: string): boolean {
  const trimmed = email.trim();
  if (trimmed.length === 0) {
    return false;
  }
  return trimmed.includes("@");
}

function validatePhone(phone: string): boolean {
  const cleaned = phone.trim();
  if (cleaned.length === 0) {
    return false;
  }
  return cleaned.includes("+");
}

// Unique function: no clone
function uniqueHelper(): number {
  let sum = 0;
  for (let i = 0; i < 100; i++) {
    sum += Math.pow(i, 2) * Math.random();
  }
  return sum;
}
```

**Module file** (`src/metrics/duplication.zig`):
Define all public types (see RESEARCH.md data structures):
- `Token` struct: `kind: []const u8`, `start_byte: u32`, `start_line: u32`
- `TokenWindow` struct: `file_index: u32`, `start_token: u32`, `end_token: u32`, `start_line: u32`, `end_line: u32`
- `CloneLocation` struct: `file_path: []const u8`, `start_line: u32`, `end_line: u32`
- `CloneGroup` struct: `token_count: u32`, `locations: []const CloneLocation`
- `FileDuplicationResult` struct: `path: []const u8`, `total_tokens: u32`, `cloned_tokens: u32`, `duplication_pct: f64`
- `DuplicationResult` struct: `clone_groups: []const CloneGroup`, `file_results: []const FileDuplicationResult`, `total_cloned_tokens: u32`, `total_tokens: u32`, `project_duplication_pct: f64`
- `DuplicationConfig` struct: `min_window: u32 = 25`, `file_warning_pct: f64 = 15.0`, `file_error_pct: f64 = 25.0`, `project_warning_pct: f64 = 5.0`, `project_error_pct: f64 = 10.0`

Define function stubs that return error or empty results (make tests compilable but failing):
- `pub fn tokenizeTree(allocator, root: tree_sitter.Node, source: []const u8) ![]Token` — returns empty slice
- `pub fn detectDuplication(allocator, file_tokens: []const FileTokens, config: DuplicationConfig) !DuplicationResult` — returns empty result
- `FileTokens` struct: `path: []const u8`, `tokens: []const Token`

Write RED tests (at least 6):
1. `test "tokenizeTree: produces tokens from simple function"` — parse fixture, tokenize, assert token count > 0
2. `test "tokenizeTree: skips comments"` — assert no token has kind "comment" or "line_comment"
3. `test "tokenizeTree: normalizes identifiers to V"` — assert identifier tokens have kind "V"
4. `test "tokenizeTree: skips type annotations"` — use a TS type-annotated function, verify type tokens excluded
5. `test "detectDuplication: finds clone groups in identical functions"` — tokenize two identical function token sequences, detect, assert at least 1 clone group
6. `test "detectDuplication: finds Type 2 clones with different identifiers"` — tokenize two structurally identical functions with different names, detect clones
7. `test "detectDuplication: merges overlapping intervals correctly"` — test that duplication_pct does not exceed the actual proportion

Add the module import to `src/main.zig` test block and `src/lib.zig` for discovery:
- In main.zig: add `_ = @import("metrics/duplication.zig");` in the test block
- In lib.zig: add `pub const duplication = @import("metrics/duplication.zig");`

Run `zig build test` — tests MUST compile but FAIL (RED phase).

Commit: `test(11-01): add failing tests for duplication tokenization and clone detection`
  </action>
  <verify>`zig build test 2>&1 | grep -c "FAIL"` shows test failures (RED). The code compiles but tests fail because stubs return empty results.</verify>
  <done>Fixture file exists with annotated duplicate functions. Duplication module has all type definitions and function stubs. At least 6 failing tests exercise tokenization, normalization, clone detection, and interval merging.</done>
</task>

<task type="auto">
  <name>Task 2: Implement tokenization, rolling hash, cross-file index, verification, and merging to pass all tests</name>
  <files>src/metrics/duplication.zig</files>
  <action>
Implement the full algorithm in `src/metrics/duplication.zig`:

**1. tokenizeTree** — Walk AST collecting leaf tokens:
- Use `tree_sitter.Node` API: `childCount()`, `child(i)`, `nodeType()`, `startPoint()`, `startByte()`
- Recurse through all children; at leaf nodes (childCount == 0), collect token
- Skip nodes with kind: `"comment"`, `"line_comment"`, `"block_comment"` (comments)
- Skip pure punctuation: `";"`, `","` (per RESEARCH.md recommendation, include `{` and `}`)
- Use `isTypeOnlyNode` pattern from halstead.zig to skip TypeScript type annotation subtrees (import the function or reimplement — check if halstead.zig exports it, if not duplicate the logic)
- Normalize identifiers: if kind is `"identifier"`, `"property_identifier"`, `"type_identifier"`, or `"shorthand_property_identifier"`, set kind to `"V"`
- Store `Token{ .kind = normalized_kind, .start_byte = node.startByte(), .start_line = node.startPoint().row + 1 }`

**2. Rolling hash** — Rabin-Karp with base 37, u64 wrapping arithmetic:
- `tokenHash(kind: []const u8) u64` — hash the kind string: `h = h *% 37 +% @as(u64, c)` for each byte
- `RollingHasher` struct with `hash: u64`, `base_pow: u64`
  - `init(tokens, window)` — compute initial hash and B^(window-1)
  - `roll(remove, add)` — sliding update: `hash = (hash -% tokenHash(remove.kind) *% base_pow) *% 37 +% tokenHash(add.kind)`

**3. Cross-file hash index** — `std.AutoHashMap(u64, std.ArrayList(TokenWindow))`:
- For each file's token sequence, if len < window skip
- Slide rolling hash window, store `TokenWindow` at each position
- Use `getOrPut` pattern for hash map

**4. Clone group formation and verification**:
- Filter buckets with >= 2 entries
- Apply maximum bucket size limit of 1000 (per RESEARCH.md pitfall 2 — discard very common patterns)
- For each bucket, verify token-by-token with `tokensMatch` comparing kind strings
- Group verified matches into `CloneGroup` with `CloneLocation` entries
- Only include groups with 2+ distinct file locations (skip same-file same-position matches)
- Also include cross-position same-file clones (copy-paste within one file)

**5. Interval merging**:
- Per file, collect all clone token intervals (start_token, end_token)
- Sort by start_token
- Merge overlapping/adjacent intervals (standard algorithm)
- Sum merged interval sizes for `cloned_tokens`

**6. detectDuplication entry point**:
- Takes `[]const FileTokens` and `DuplicationConfig`
- Build hash index from all files
- Form and verify clone groups
- Merge intervals per file
- Compute per-file `FileDuplicationResult` (duplication_pct = cloned_tokens / total_tokens * 100)
- Compute project totals
- Return `DuplicationResult`

**Memory management:**
- Use the provided allocator for all ArrayList and HashMap operations
- Pass allocator to all `.append()`, `.deinit()`, `.getOrPut()` calls (Zig 0.15 pattern)
- After clone groups are formed, clean up the hash index (deinit all ArrayList values, then deinit the HashMap)

Run `zig build test` — ALL tests MUST pass (GREEN phase).

Commit: `feat(11-01): implement Rabin-Karp duplication detection with tokenization and cross-file indexing`
  </action>
  <verify>`zig build test` passes with zero failures. All 6+ duplication tests pass: tokenization produces tokens, comments are skipped, identifiers normalized, clone groups detected for both Type 1 and Type 2 patterns, duplication percentages are reasonable (not >100%).</verify>
  <done>Complete duplication detection algorithm: tokenizeTree extracts normalized tokens from AST, detectDuplication finds clone groups across files using Rabin-Karp rolling hash with token-by-token verification and interval merging. All tests pass.</done>
</task>

</tasks>

<verification>
1. `zig build test` — all tests pass (including duplication module tests)
2. `zig build` — binary compiles successfully
3. Module correctly tokenizes TypeScript ASTs, normalizes identifiers, and detects both Type 1 and Type 2 clones
4. Overlapping intervals are merged (no duplication_pct > 100%)
5. Hash collision verification prevents false clone groups
</verification>

<success_criteria>
- src/metrics/duplication.zig exists with tokenizeTree and detectDuplication functions
- tests/fixtures/typescript/duplication_cases.ts exists with annotated clone blocks
- All duplication tests pass via `zig build test`
- Module imported in main.zig test block and lib.zig
- TDD commits: RED (failing tests) then GREEN (passing implementation)
</success_criteria>

<output>
After completion, create `.planning/phases/11-duplication-detection/11-01-SUMMARY.md`
</output>
